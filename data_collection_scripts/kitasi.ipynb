{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4b5d20",
   "metadata": {},
   "source": [
    "# USAID FUNDING SENTIMENTAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b933cc7",
   "metadata": {},
   "source": [
    "# Reddit Data Extraction with the Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id='fgIEgG6e6I_bFHDqpWQYdQ',\n",
    "    client_secret='qd7w7BaeDRr6L7AvK3H4AYAr-AENsA',\n",
    "    user_agent='Ill-Chocolate-4761'\n",
    ")\n",
    "\n",
    "subreddits = ['Kenya', 'Africa', 'EastAfrica']\n",
    "queries = [\n",
    "    \"USAID funding\"\n",
    "]\n",
    "\n",
    "data = []\n",
    "\n",
    "#Loop through subreddits and collect matching posts\n",
    "for sub in subreddits:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    print(f\"Fetching new posts from r/{sub}\")\n",
    "    for post in subreddit.new(limit=1000):\n",
    "        post_text = f\"{post.title} {post.selftext}\".lower()\n",
    "        if any(q.lower() in post_text for q in queries):\n",
    "            data.append({\n",
    "                'subreddit': sub,\n",
    "                'title': post.title,\n",
    "                'score': post.score,\n",
    "                'url': post.url,\n",
    "                'created_utc': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'num_comments': post.num_comments,\n",
    "                'selftext': post.selftext\n",
    "            })\n",
    "        if len(data) >= 200:\n",
    "            break  # Stop after collecting 100 matches\n",
    "\n",
    "# Create DataFrame and save\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('reddit_usaid_sentiment_sample.csv', index=False)\n",
    "print(f\"Saved {len(df)} matching posts to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c166ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r\"C:\\Users\\AHB\\Desktop\\my_quick_acess\\3.Projects\\projects pipeline\\2025\\USAID-Kenya-Sentiment-Analysis\\data\\reddit_kitasi2.csv\"\n",
    "df.to_csv(r\"C:\\Users\\AHB\\Desktop\\my_quick_acess\\3.Projects\\projects pipeline\\2025\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\ruth_reddit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d09d7",
   "metadata": {},
   "source": [
    "# Extracting data from news using News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install newspaper3k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55da3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from newspaper import Article  # New import\n",
    "import time\n",
    "\n",
    "# Your existing setup\n",
    "api_key = '4ee66544d2954e7facf1b04e48b55ee3'\n",
    "query = 'USAID funding Kenya'\n",
    "url = f'https://newsapi.org/v2/everything?q={query}&language=en&sortBy=publishedAt&pageSize=100&apiKey={api_key}'\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "articles = data.get('articles', [])\n",
    "\n",
    "# Build the initial DataFrame\n",
    "df_news = pd.DataFrame([{\n",
    "    'source': article['source']['name'],\n",
    "    'title': article['title'],\n",
    "    'description': article['description'],\n",
    "    'url': article['url'],\n",
    "    'publishedAt': article['publishedAt']\n",
    "} for article in articles])\n",
    "\n",
    "# ✅ Use newspaper3k to scrape full content\n",
    "contents = []\n",
    "for link in df_news['url']:\n",
    "    try:\n",
    "        article = Article(link)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        contents.append(article.text)\n",
    "    except Exception as e:\n",
    "        contents.append(None)  # or str(e) if you want error messages\n",
    "    time.sleep(1)  # Optional, to be polite and avoid blocking\n",
    "        \n",
    "\n",
    "# Add the full text to the DataFrame\n",
    "df_news[\"content_full\"] = contents\n",
    "\n",
    "# ✅ Inspect the results\n",
    "print(df_news.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f42212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.to_csv(r\"C:\\Users\\AHB\\Desktop\\my_quick_acess\\3.Projects\\projects pipeline\\2025\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\ruth__news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Your API Key\n",
    "api_key = '4ee66544d2954e7facf1b04e48b55ee3' \n",
    "\n",
    "# Query Parameters\n",
    "query = 'USAID funding Kenya'\n",
    "url = f'https://newsapi.org/v2/everything?q={query}&language=en&sortBy=publishedAt&pageSize=100&apiKey={api_key}'\n",
    "\n",
    "# Make request\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Convert to DataFrame\n",
    "articles = data['articles']\n",
    "\n",
    "df_news = pd.DataFrame([{\n",
    "    'source': article['source']['name'],\n",
    "    'title': article['title'],\n",
    "    'description': article['description'],\n",
    "    'content': article['content'],\n",
    "    'url': article['url'],\n",
    "    'publishedAt': article['publishedAt']\n",
    "} for article in articles])\n",
    "\n",
    "# Show sample data\n",
    "df_news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.to_csv(r\"C:\\Users\\AHB\\Desktop\\my_quick_acess\\3.Projects\\projects pipeline\\2025\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\ruth_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be21f03",
   "metadata": {},
   "source": [
    "# Extracting data from Twitter using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f60341",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Replace with your actual Bearer Token\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAK4%2F2gEAAAAA1TH8z2qhYiK850CchE07JVIULK0%3Dr0WqcoK6o1lrW90sqbDhRtMlI32NkfurzPIr13Tte5nu3DziV1\"\n",
    "\n",
    "# Connect to Twitter API\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# Search tweets containing the word \"Kenya\"\n",
    "query = \"USAID funding kenya\"\n",
    "tweets = client.search_recent_tweets(query=query, max_results=10)\n",
    "\n",
    "# Print the tweets\n",
    "for tweet in tweets.data:\n",
    "    print(tweet.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "tweet_data = []\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    tweet_data.append({\n",
    "        \"id\": tweet.id,\n",
    "        \"text\": tweet.text,\n",
    "        \"created_at\": tweet.created_at  # may need to add 'tweet.fields' param to see this\n",
    "    })\n",
    "\n",
    "df_tweets = pd.DataFrame(tweet_data)\n",
    "\n",
    "# Now you can use .head()\n",
    "df_tweets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets.to_csv(r\"C:\\Users\\AHB\\Desktop\\my_quick_acess\\3.Projects\\projects pipeline\\2025\\USAID-Kenya-Sentiment-Analysis\\data\\news_data.csv\")\n",
    "df_tweets.to_csv(r\"C:\\Users\\AHB\\Desktop\\my_quick_acess\\3.Projects\\projects pipeline\\2025\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\ruth_tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fc338",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
