{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47578532",
   "metadata": {},
   "source": [
    "# Data Collection Experimentation\n",
    "- The announcement of the USAID funding cuts was made on March 28, 2025\n",
    "## 1. Environments\n",
    "- tweepy, PRAW requests (pip install)\n",
    "\n",
    "## 2. Reddit Data collection\n",
    "- Create connection with Reddit: [link to create script->(https://www.reddit.com/prefs/apps)](https://www.reddit.com/prefs/apps)\n",
    "- Get `client_id`, `client_secret`\n",
    "```\n",
    "reddit = praw.Reddit(\n",
    "    client_id='tWd763iMgmjp8YamFF96Wg',\n",
    "    client_secret='\tXmaO9oO_kioW-8DzCLipxz-A4hffSA',\n",
    "    user_agent='usaid-sentiment-KE'\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dd337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching r/Kenya...\n",
      "Searching r/EastAfrica...\n",
      "‚úÖ Scraped 24 posts. Saved to ../data/raw/leo_reddit_posts.csv'.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='tWd763iMgmjp8YamFF96Wg',\n",
    "    client_secret='XmaO9oO_kioW-8DzCLipxz-A4hffSA',\n",
    "    user_agent='usaid-sentiment-KE'\n",
    ")\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "subreddits = ['Kenya', 'EastAfrica']\n",
    "\n",
    "\"\"\"keywords = [\n",
    "    \"USAID\", \"usaid\", \n",
    "    \"foreign aid\", \"foreign assistance\", \n",
    "    \"donor\", \"donour\", \n",
    "    \"funding\", \"funds\", \n",
    "    \"budget cuts\", \"aid cuts\", \n",
    "    \"development aid\", \n",
    "    \"healthcare\", \"health care\", \n",
    "    \"NGOs\", \"nonprofits\", \"non-profits\"\n",
    "]\"\"\"\n",
    "\n",
    "\n",
    "keywords = [\n",
    "    \"USAID\", \"usaid\", \n",
    "    \"foreign aid\", \"foreign funding\"  \n",
    "]\n",
    "\n",
    "# Combine keywords and phrases\n",
    "search_terms = keywords\n",
    "\n",
    "# Earliest date (after funding cuts) ‚Üí March 28, 2025\n",
    "cutoff_date = datetime(2025, 3, 28, tzinfo=timezone.utc).timestamp()\n",
    "\n",
    "# --- SCRAPING ---\n",
    "data = []\n",
    "\n",
    "for sub in subreddits:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    print(f\"Searching r/{sub}...\")\n",
    "    \n",
    "    for term in search_terms:\n",
    "        try:\n",
    "            for post in subreddit.search(term, sort='new', limit=200):\n",
    "                if post.created_utc < cutoff_date:\n",
    "                    continue  # skip posts before March 28, 2025\n",
    "                \n",
    "                data.append({\n",
    "                    'subreddit': sub,\n",
    "                    'search_term': term,\n",
    "                    'title': post.title,\n",
    "                    'text': post.selftext,\n",
    "                    'created_utc': post.created_utc,\n",
    "                    'created_date': datetime.fromtimestamp(post.created_utc),\n",
    "                    'score': post.score,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'permalink': f\"https://reddit.com{post.permalink}\",\n",
    "                    'url': post.url\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching term '{term}' in r/{sub}: {e}\")\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(data)\n",
    "df['created_date'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "df.to_csv('../data/raw/leo_reddit_posts.csv', index=False)\n",
    "\n",
    "print(f\" Scraped {len(df)} posts. Saved to ../data/raw/leo_reddit_posts.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565c8a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>search_term</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_date</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>usaid</td>\n",
       "      <td>Economy</td>\n",
       "      <td>For the experts in matters economy and finance...</td>\n",
       "      <td>1.743959e+09</td>\n",
       "      <td>2025-04-06 17:01:41</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1jsytyp/ec...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsyty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign funding</td>\n",
       "      <td>Be very cautious of the UAE</td>\n",
       "      <td>Kasongo has been cozying up to the UAE recentl...</td>\n",
       "      <td>1.748512e+09</td>\n",
       "      <td>2025-05-29 09:51:51</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1ky6sma/be...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1ky6sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>USAID</td>\n",
       "      <td>USAID Repercussions + Economy</td>\n",
       "      <td>My neighbour‚Äôs wife was a very big shot in USA...</td>\n",
       "      <td>1.747235e+09</td>\n",
       "      <td>2025-05-14 15:11:02</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1kmhn87/us...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1kmhn8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign funding</td>\n",
       "      <td>Daily Nation</td>\n",
       "      <td></td>\n",
       "      <td>1.747811e+09</td>\n",
       "      <td>2025-05-21 07:09:24</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1krrnpb/da...</td>\n",
       "      <td>https://www.reddit.com/gallery/1krrnpb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign aid</td>\n",
       "      <td>Is There a Better Way to Fund Africa‚Äôs Infrast...</td>\n",
       "      <td>I'm researching a fintech concept rooted in a ...</td>\n",
       "      <td>1.745161e+09</td>\n",
       "      <td>2025-04-20 14:49:50</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1k3o7to/is...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1k3o7t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>usaid</td>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>1.743880e+09</td>\n",
       "      <td>2025-04-05 19:09:10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1jsb149/ex...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsb14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign aid</td>\n",
       "      <td>Kibaki ALSO failed us</td>\n",
       "      <td>\\nThere is a tendency to over-exaggerate the p...</td>\n",
       "      <td>1.743470e+09</td>\n",
       "      <td>2025-04-01 01:12:42</td>\n",
       "      <td>120</td>\n",
       "      <td>124</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1jojl2f/ki...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jojl2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>USAID</td>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>1.744723e+09</td>\n",
       "      <td>2025-04-15 13:16:53</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1jzrn2s/us...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jzrn2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign funding</td>\n",
       "      <td>Like It or Not, Here's Why Ruto Will Win in 20...</td>\n",
       "      <td>CALL ME WHATEVER YOU WANT, BUT HERE'S THE BARE...</td>\n",
       "      <td>1.745140e+09</td>\n",
       "      <td>2025-04-20 09:06:35</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1k3ijdu/li...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1k3ijd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>usaid</td>\n",
       "      <td>USAID Repercussions + Economy</td>\n",
       "      <td>My neighbour‚Äôs wife was a very big shot in USA...</td>\n",
       "      <td>1.747235e+09</td>\n",
       "      <td>2025-05-14 15:11:02</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1kmhn87/us...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1kmhn8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit      search_term  \\\n",
       "8      Kenya            usaid   \n",
       "16     Kenya  foreign funding   \n",
       "0      Kenya            USAID   \n",
       "18     Kenya  foreign funding   \n",
       "11     Kenya      foreign aid   \n",
       "9      Kenya            usaid   \n",
       "13     Kenya      foreign aid   \n",
       "1      Kenya            USAID   \n",
       "21     Kenya  foreign funding   \n",
       "5      Kenya            usaid   \n",
       "\n",
       "                                                title  \\\n",
       "8                                             Economy   \n",
       "16                        Be very cautious of the UAE   \n",
       "0                       USAID Repercussions + Economy   \n",
       "18                                       Daily Nation   \n",
       "11  Is There a Better Way to Fund Africa‚Äôs Infrast...   \n",
       "9                        EX-USAID people!! Let's talk   \n",
       "13                              Kibaki ALSO failed us   \n",
       "1   USAID left a month ago, do we have ARVs in Kenya?   \n",
       "21  Like It or Not, Here's Why Ruto Will Win in 20...   \n",
       "5                       USAID Repercussions + Economy   \n",
       "\n",
       "                                                 text   created_utc  \\\n",
       "8   For the experts in matters economy and finance...  1.743959e+09   \n",
       "16  Kasongo has been cozying up to the UAE recentl...  1.748512e+09   \n",
       "0   My neighbour‚Äôs wife was a very big shot in USA...  1.747235e+09   \n",
       "18                                                     1.747811e+09   \n",
       "11  I'm researching a fintech concept rooted in a ...  1.745161e+09   \n",
       "9   Are you still in contact with the organisation...  1.743880e+09   \n",
       "13  \\nThere is a tendency to over-exaggerate the p...  1.743470e+09   \n",
       "1   Someone on a different group (different websit...  1.744723e+09   \n",
       "21  CALL ME WHATEVER YOU WANT, BUT HERE'S THE BARE...  1.745140e+09   \n",
       "5   My neighbour‚Äôs wife was a very big shot in USA...  1.747235e+09   \n",
       "\n",
       "          created_date  score  num_comments  \\\n",
       "8  2025-04-06 17:01:41      1            15   \n",
       "16 2025-05-29 09:51:51     32            13   \n",
       "0  2025-05-14 15:11:02     12            32   \n",
       "18 2025-05-21 07:09:24      1             8   \n",
       "11 2025-04-20 14:49:50      9             9   \n",
       "9  2025-04-05 19:09:10      2             0   \n",
       "13 2025-04-01 01:12:42    120           124   \n",
       "1  2025-04-15 13:16:53      3             5   \n",
       "21 2025-04-20 09:06:35      0             9   \n",
       "5  2025-05-14 15:11:02     13            32   \n",
       "\n",
       "                                            permalink  \\\n",
       "8   https://reddit.com/r/Kenya/comments/1jsytyp/ec...   \n",
       "16  https://reddit.com/r/Kenya/comments/1ky6sma/be...   \n",
       "0   https://reddit.com/r/Kenya/comments/1kmhn87/us...   \n",
       "18  https://reddit.com/r/Kenya/comments/1krrnpb/da...   \n",
       "11  https://reddit.com/r/Kenya/comments/1k3o7to/is...   \n",
       "9   https://reddit.com/r/Kenya/comments/1jsb149/ex...   \n",
       "13  https://reddit.com/r/Kenya/comments/1jojl2f/ki...   \n",
       "1   https://reddit.com/r/Kenya/comments/1jzrn2s/us...   \n",
       "21  https://reddit.com/r/Kenya/comments/1k3ijdu/li...   \n",
       "5   https://reddit.com/r/Kenya/comments/1kmhn87/us...   \n",
       "\n",
       "                                                  url  \n",
       "8   https://www.reddit.com/r/Kenya/comments/1jsyty...  \n",
       "16  https://www.reddit.com/r/Kenya/comments/1ky6sm...  \n",
       "0   https://www.reddit.com/r/Kenya/comments/1kmhn8...  \n",
       "18             https://www.reddit.com/gallery/1krrnpb  \n",
       "11  https://www.reddit.com/r/Kenya/comments/1k3o7t...  \n",
       "9   https://www.reddit.com/r/Kenya/comments/1jsb14...  \n",
       "13  https://www.reddit.com/r/Kenya/comments/1jojl2...  \n",
       "1   https://www.reddit.com/r/Kenya/comments/1jzrn2...  \n",
       "21  https://www.reddit.com/r/Kenya/comments/1k3ijd...  \n",
       "5   https://www.reddit.com/r/Kenya/comments/1kmhn8...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(random_state=42, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccf7e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>search_term</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_date</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>usaid</td>\n",
       "      <td>Economy</td>\n",
       "      <td>For the experts in matters economy and finance...</td>\n",
       "      <td>1.743959e+09</td>\n",
       "      <td>2025-04-06 17:01:41</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1jsytyp/ec...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsyty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign funding</td>\n",
       "      <td>Be very cautious of the UAE</td>\n",
       "      <td>Kasongo has been cozying up to the UAE recentl...</td>\n",
       "      <td>1.748512e+09</td>\n",
       "      <td>2025-05-29 09:51:51</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1ky6sma/be...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1ky6sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>USAID</td>\n",
       "      <td>USAID Repercussions + Economy</td>\n",
       "      <td>My neighbour‚Äôs wife was a very big shot in USA...</td>\n",
       "      <td>1.747235e+09</td>\n",
       "      <td>2025-05-14 15:11:02</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1kmhn87/us...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1kmhn8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign funding</td>\n",
       "      <td>Daily Nation</td>\n",
       "      <td></td>\n",
       "      <td>1.747811e+09</td>\n",
       "      <td>2025-05-21 07:09:24</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1krrnpb/da...</td>\n",
       "      <td>https://www.reddit.com/gallery/1krrnpb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kenya</td>\n",
       "      <td>foreign aid</td>\n",
       "      <td>Is There a Better Way to Fund Africa‚Äôs Infrast...</td>\n",
       "      <td>I'm researching a fintech concept rooted in a ...</td>\n",
       "      <td>1.745161e+09</td>\n",
       "      <td>2025-04-20 14:49:50</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>https://reddit.com/r/Kenya/comments/1k3o7to/is...</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1k3o7t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit      search_term  \\\n",
       "8      Kenya            usaid   \n",
       "16     Kenya  foreign funding   \n",
       "0      Kenya            USAID   \n",
       "18     Kenya  foreign funding   \n",
       "11     Kenya      foreign aid   \n",
       "\n",
       "                                                title  \\\n",
       "8                                             Economy   \n",
       "16                        Be very cautious of the UAE   \n",
       "0                       USAID Repercussions + Economy   \n",
       "18                                       Daily Nation   \n",
       "11  Is There a Better Way to Fund Africa‚Äôs Infrast...   \n",
       "\n",
       "                                                 text   created_utc  \\\n",
       "8   For the experts in matters economy and finance...  1.743959e+09   \n",
       "16  Kasongo has been cozying up to the UAE recentl...  1.748512e+09   \n",
       "0   My neighbour‚Äôs wife was a very big shot in USA...  1.747235e+09   \n",
       "18                                                     1.747811e+09   \n",
       "11  I'm researching a fintech concept rooted in a ...  1.745161e+09   \n",
       "\n",
       "          created_date  score  num_comments  \\\n",
       "8  2025-04-06 17:01:41      1            15   \n",
       "16 2025-05-29 09:51:51     32            13   \n",
       "0  2025-05-14 15:11:02     12            32   \n",
       "18 2025-05-21 07:09:24      1             8   \n",
       "11 2025-04-20 14:49:50      9             9   \n",
       "\n",
       "                                            permalink  \\\n",
       "8   https://reddit.com/r/Kenya/comments/1jsytyp/ec...   \n",
       "16  https://reddit.com/r/Kenya/comments/1ky6sma/be...   \n",
       "0   https://reddit.com/r/Kenya/comments/1kmhn87/us...   \n",
       "18  https://reddit.com/r/Kenya/comments/1krrnpb/da...   \n",
       "11  https://reddit.com/r/Kenya/comments/1k3o7to/is...   \n",
       "\n",
       "                                                  url  \n",
       "8   https://www.reddit.com/r/Kenya/comments/1jsyty...  \n",
       "16  https://www.reddit.com/r/Kenya/comments/1ky6sm...  \n",
       "0   https://www.reddit.com/r/Kenya/comments/1kmhn8...  \n",
       "18             https://www.reddit.com/gallery/1krrnpb  \n",
       "11  https://www.reddit.com/r/Kenya/comments/1k3o7t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = df.sample(n=5,random_state= 42)\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfa1737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the experts in matters economy and finance I ask this politely(mnielezee Kama mtoto tafadhali). How is our country still semi functional? Everyday we hear cases of billions lost here billions lost there. Sometime there was reports of I think 1.3 trillion irregularly withdrawn from the treasury, the dollar has surprisingly been stable at around 129 despite all this and there was the case where funding would be halted by the USAID. How has the economy not crashed yet? Is it normal to lose a third of the budget and still have a running country?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Kasongo has been cozying up to the UAE recently and as Kenyans we should be very careful here, if you look at their foreign policy they a pattern of fostering chaos and undermining democracy and legitimate governments.\\n\\n- In Sudan they fund and support the RSF ,in fact there are reports that they are the ones who pushed the RSF into launching the war.\\n- In Somalia they support the breakaway region of Somaliland.\\n- In Libya they fund and support the warlord, Khalifa Haftar.\\n- In Egypt they orchestrated a coup to overthrow Morsy, the only democratically elected leader in Egypt.\\n\\nI don't know but who's to say that they will not try and help Kasongo in subverting the 2027 elections? After all they wouldn't wanna lose their logistics hub.  As the Swahili say 'Ukiona cha mwenzako kinanyolewa ,chako tia maji' and btw all those countries you see above all thought it couldn't happen to them.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'My neighbour‚Äôs wife was a very big shot in USAID and has now lost her job. Children have been removed from big private school. Husband is a big guy at PWC. Lifestyle changes are occurring rapidly as her income has vanished. Thousands of her USAID coworkers were sent home with no salaries. \\n\\nUSAID Vendors, contractors, non-profits that received funding from them have all been left in a lurch. Sasa machozi zimeanza. \\n\\nNext is empty apartments around ‚Äúhigh class‚Äù areas.\\n\\nUN is laying people off left right and center.\\n\\nAdditionally, public assistance programs in the Europe and America are being slashed so remittances by a certain sector are falling.\\n\\nIf you think things are hard, ngojeni mpaka December. A lot of your highlife hotspots are about to close. A lot of these restaurants are about to close. \\n\\nCrime shall return so please rudini mashambani mulime.\\n\\nAvoid Mombasa, Lamu and malls.\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m researching a fintech concept rooted in a simple but powerful idea: What if African citizens could directly micro-invest in their own infrastructure and economic development ‚Äî from as little as $1 ‚Äî instead of relying so heavily on foreign loans or aid?\\n\\nThe idea is inspired by:\\n\\nEthiopia\\'s Renaissance Dam, where despite China funding most of the $5B project, citizens contributed around $1B through bonds and mobile payments. It was a unifying act of nation-building.\\n\\nDenmark‚Äôs wind cooperatives, where tens of thousands of Danes co-own wind turbines, investing small amounts and earning steady returns from green energy sales.\\n\\nArla Foods, one of the world‚Äôs largest dairy companies, is owned by thousands of farmer-members across Europe.\\n\\nPark Slope Food Co-op (Brooklyn, USA) ‚Äì over 17,000 members run and own this highly successful grocery store. Members contribute labor and share in decision-making and cost savings ‚Äî a small-scale but high-functioning democratic economic model.\\n\\nThe concept:\\n\\nA micro-investment platform where citizens can fund infrastructure and industrial projects such as:\\n\\nSolar mini-grids\\n\\nRoads, ports, water systems\\n\\nLocal processing plants or factories\\n\\nAffordable housing\\n\\nAgricultural or logistics ventures\\n\\n\\nUsers invest tiny amounts (e.g. $1‚Äì$10) and track the project‚Äôs progress. They may receive a return over time or non-cash benefits (e.g. discounts, usage credits).\\n\\n\\nWhy this matters:\\n\\nToo often, African development is externally financed ‚Äî with debt, strings attached, and little citizen engagement. This model flips that:\\n\\nPeople co-own what they rely on\\n\\nGovernments gain domestic funding alternatives\\n\\nTrust, pride, and engagement are built from the ground up\\n\\n\\nChallenges (based on Reddit and expert feedback):\\n\\n1. Corruption and trust ‚Äî Citizens must see where every dollar goes. This means transparent ledgers, project dashboards, public audits, and perhaps smart contracts.\\n\\n\\n2. Regulation hell ‚Äî Securities laws differ by country. Government support or sandbox frameworks would be key.\\n\\n\\n3. Profitability ‚Äî Many infrastructure projects don‚Äôt generate immediate returns. The model may need to combine financial ROI with social ROI (access, pride, service).\\n\\n\\n4. Liquidity and exits ‚Äî Who buys your stake in a toll road if you need cash tomorrow?\\n\\n\\n5. \"Isn‚Äôt this just a tax?\" ‚Äî Not quite. Unlike taxes, citizens choose projects and can receive returns or benefits.\\n\\n\\nWhat I‚Äôm exploring:\\n\\nStarting with small-scale, single-country pilots (e.g. local solar or transport infrastructure)\\n\\nIntegrating traditional savings models like stokvels or SACCOs for community-level buy-in\\n\\nBuilding a trust layer first: partnerships with co-ops, municipalities, development banks, etc.\\n\\nExploring hybrid returns (financial + utility discounts) and different legal structures (co-ops, trusts, SPVs)\\n\\n\\nI\\'m not claiming this is the silver bullet ‚Äî but I do believe there\\'s space for a new model of citizen-led development funding in Africa.\\n\\nWhat are the biggest red flags? Where does this break down? Are there other models you think I should study or emulate?\\n\\nI‚Äôd love to hear your take.\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text= sample['text'].to_list()\n",
    "for x in sample_text:\n",
    "    display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac31c76",
   "metadata": {},
   "source": [
    "- As seen above, some texts in our dataset contains mixed languages; we shall therefore have some preprocessing before analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b7fe4",
   "metadata": {},
   "source": [
    "## 3. NewsAPI Data Collection\n",
    "- Sign up at [newsapi.org](newsapi.org)\n",
    "- Create account and get api key : `bc6c52fd05ee4e63827b7cf45fa0bdb2`\n",
    "- Limitations\n",
    "    - The free version allows me to search back until 2025-05-03, 2 months after USAID funding cuts were already announced\n",
    "    - Paywalled Websites like Daily Nation won't have content\n",
    "    - Limited to 100 results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb696a",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb18d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: 426 - {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-05-12, but you have requested 2025-05-04. You may need to upgrade to a paid plan.'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'published_at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'published_at'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc4c350ad008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# --- SAVE TO CSV ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'published_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'published_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/leo_newsapi_articles.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'published_at'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "api_key = 'bc6c52fd05ee4e63827b7cf45fa0bdb2'\n",
    "query = 'USAID'\n",
    "from_date = '2025-05-04'  # YYYY-MM-DD\n",
    "country = 'ke'  # Kenya\n",
    "page_size = 100  # max per request\n",
    "max_pages = 1    # you can loop through more if needed\n",
    "\n",
    "# --- FETCH ARTICLES ---\n",
    "all_articles = []\n",
    "\n",
    "for page in range(1, max_pages + 1):\n",
    "    url = (\n",
    "        f'https://newsapi.org/v2/everything?'\n",
    "        f'q={query}&'\n",
    "        f'from={from_date}&'\n",
    "        f'sortBy=publishedAt&'\n",
    "        f'pageSize={page_size}&'\n",
    "        f'page={page}&'\n",
    "        f'apiKey={api_key}'\n",
    "    )\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Error: {response.status_code} - {response.json()}\")\n",
    "        break\n",
    "\n",
    "    articles = response.json().get('articles', [])\n",
    "    if not articles:\n",
    "        break  # no more results\n",
    "\n",
    "    for article in articles:\n",
    "        all_articles.append({\n",
    "            'source': article['source']['name'],\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'content': article.get('content'),\n",
    "            'url': article.get('url'),\n",
    "            'published_at': article.get('publishedAt')\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(all_articles)\n",
    "# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(all_articles)\n",
    "df['published_at'] = pd.to_datetime(df['published_at'])\n",
    "df.to_csv('../data/raw/leo_newsapi_articles.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Fetched {len(df)} articles. Saved to ../data/raw/leo_newsapi_articles.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132d5bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   source        99 non-null     object\n",
      " 1   author        94 non-null     object\n",
      " 2   title         99 non-null     object\n",
      " 3   description   98 non-null     object\n",
      " 4   content       98 non-null     object\n",
      " 5   url           97 non-null     object\n",
      " 6   published_at  97 non-null     object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_news = pd.read_csv('../data/raw/leo_newsapi_articles.csv')\n",
    "df_news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d20e96",
   "metadata": {},
   "source": [
    "-Enrich NewsAPI Data with Full Text from `newspaper3k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a93c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 99 articles from NewsAPI.\n",
      "‚ùå Failed at index 11 (https://www.foxnews.com/us/boulder-suspect-spent-year-planning-molotov-cocktail-attack-pro-israel-march-docs): Article `download()` failed with 404 Client Error: Not Found for url: https://www.foxnews.com/us/boulder-suspect-spent-year-planning-molotov-cocktail-attack-pro-israel-march-docs on URL https://www.foxnews.com/us/boulder-suspect-spent-year-planning-molotov-cocktail-attack-pro-israel-march-docs\n",
      "‚ùå Failed at index 13 (https://www.foxnews.com/us/usaid-paperwork-found-car-boulder-terror-attack-suspect-targeting-pro-israel-group): Article `download()` failed with 404 Client Error: Not Found for url: https://www.foxnews.com/us/usaid-paperwork-found-car-boulder-terror-attack-suspect-targeting-pro-israel-group on URL https://www.foxnews.com/us/usaid-paperwork-found-car-boulder-terror-attack-suspect-targeting-pro-israel-group\n",
      "‚ùå Failed at index 15 (https://www.abc.net.au/news/2025-06-03/colorado-terror-attack-suspect-charged-with-us-hate-crime/105369384): Article `download()` failed with 403 Client Error: Forbidden for url: https://www.abc.net.au/news/2025-06-03/colorado-terror-attack-suspect-charged-with-us-hate-crime/105369384 on URL https://www.abc.net.au/news/2025-06-03/colorado-terror-attack-suspect-charged-with-us-hate-crime/105369384\n",
      "‚ùå Failed at index 16 (nan): 'float' object has no attribute 'decode'\n",
      "‚ùå Failed at index 17 (nan): 'float' object has no attribute 'decode'\n",
      "‚ùå Failed at index 28 (https://www.euractiv.com/section/economy-jobs/news/germany-puts-shrunken-development-aid-budget-to-the-test/): Article `download()` failed with 403 Client Error: Forbidden for url: https://www.euractiv.com/section/economy-jobs/news/germany-puts-shrunken-development-aid-budget-to-the-test/ on URL https://www.euractiv.com/section/economy-jobs/news/germany-puts-shrunken-development-aid-budget-to-the-test/\n",
      "‚ùå Failed at index 30 (https://www.foxnews.com/us/boulder-terror-attack-suspect-said-he-wanted-kill-all-zionist-people-used-molotov-cocktails-feds): Article `download()` failed with 404 Client Error: Not Found for url: https://www.foxnews.com/us/boulder-terror-attack-suspect-said-he-wanted-kill-all-zionist-people-used-molotov-cocktails-feds on URL https://www.foxnews.com/us/boulder-terror-attack-suspect-said-he-wanted-kill-all-zionist-people-used-molotov-cocktails-feds\n",
      "‚ùå Failed at index 33 (https://www.globalresearch.ca/intro-chapter-twilight-american-imperialism/5889249): Article `download()` failed with 403 Client Error: Forbidden for url: https://www.globalresearch.ca/intro-chapter-twilight-american-imperialism/5889249 on URL https://www.globalresearch.ca/intro-chapter-twilight-american-imperialism/5889249\n",
      "‚ùå Failed at index 37 (https://www.globalresearch.ca/soon-truth-too-costly-paul-c-roberts/5889186): Article `download()` failed with 403 Client Error: Forbidden for url: https://www.globalresearch.ca/soon-truth-too-costly-paul-c-roberts/5889186 on URL https://www.globalresearch.ca/soon-truth-too-costly-paul-c-roberts/5889186\n",
      "‚ùå Failed at index 49 (https://www.foxnews.com/media/ny-times-columnist-compares-elon-musk-historys-worst-murderers-over-usaid-cuts): Article `download()` failed with 404 Client Error: Not Found for url: https://www.foxnews.com/media/ny-times-columnist-compares-elon-musk-historys-worst-murderers-over-usaid-cuts on URL https://www.foxnews.com/media/ny-times-columnist-compares-elon-musk-historys-worst-murderers-over-usaid-cuts\n",
      "‚ùå Failed at index 66 (https://www.foxnews.com/media/bonos-300000-dead-claim-over-usaid-cuts-gets-smacked-down-rogan-musk-liar-idiot): Article `download()` failed with 404 Client Error: Not Found for url: https://www.foxnews.com/media/bonos-300000-dead-claim-over-usaid-cuts-gets-smacked-down-rogan-musk-liar-idiot on URL https://www.foxnews.com/media/bonos-300000-dead-claim-over-usaid-cuts-gets-smacked-down-rogan-musk-liar-idiot\n",
      "‚ùå Failed at index 81 (https://www.newsweek.com/bono-sparks-maga-backlash-during-joe-rogan-appearance-2079295): Article `download()` failed with 403 Client Error: Forbidden for url: https://www.newsweek.com/bono-sparks-maga-backlash-during-joe-rogan-appearance-2079295 on URL https://www.newsweek.com/bono-sparks-maga-backlash-during-joe-rogan-appearance-2079295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/leo/anaconda3/envs/learn-env/lib/python3.8/site-packages/jieba/dict.txt ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.4312019348144531 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. Saved full-text articles to ../data/raw/leo_newsapi_articles_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION (if not done) ---\n",
    "# pip install newspaper3k pandas\n",
    "\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "# --- LOAD EXISTING DATA ---\n",
    "df = pd.read_csv('../data/raw/leo_newsapi_articles.csv')\n",
    "print(f\"üìÑ Loaded {len(df)} articles from NewsAPI.\")\n",
    "\n",
    "# --- EXTRACT FULL TEXT FROM URL ---\n",
    "full_texts = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        full_texts.append(article.text)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed at index {i} ({url}): {e}\")\n",
    "        full_texts.append(None)\n",
    "    \n",
    "    time.sleep(1)  # to avoid IP blocks or throttling\n",
    "\n",
    "# --- ADD TO DATAFRAME & SAVE ---\n",
    "df['full_text'] = full_texts\n",
    "df.to_csv('../data/raw/leo_newsapi_articles_enriched.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Done. Saved full-text articles to ../data/raw/leo_newsapi_articles_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3f5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c194bfa3",
   "metadata": {},
   "source": [
    "- Trying out alternative news sources due to limitations of `NewsAPI`\n",
    "## News Collection using Gnews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5784fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching from 2025-03-28 to 2025-04-04\n",
      "   ‚úÖ Found 0 articles\n",
      "üîç Searching from 2025-04-04 to 2025-04-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/envs/learn-env/lib/python3.8/site-packages/gnews/gnews.py:130: UserWarning: The start and end dates should be at least 1 day apart, or GNews will return no results\n",
      "  warnings.warn(\"The start and end dates should be at least 1 day apart, or GNews will return no results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Found 14 articles\n",
      "üîç Searching from 2025-04-11 to 2025-04-18\n",
      "   ‚úÖ Found 10 articles\n",
      "üîç Searching from 2025-04-18 to 2025-04-25\n",
      "   ‚úÖ Found 24 articles\n",
      "üîç Searching from 2025-04-25 to 2025-05-02\n",
      "   ‚úÖ Found 36 articles\n",
      "üîç Searching from 2025-05-02 to 2025-05-09\n",
      "   ‚úÖ Found 31 articles\n",
      "üîç Searching from 2025-05-09 to 2025-05-16\n",
      "   ‚úÖ Found 28 articles\n",
      "üîç Searching from 2025-05-16 to 2025-05-23\n",
      "   ‚úÖ Found 30 articles\n",
      "üîç Searching from 2025-05-23 to 2025-05-30\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç Searching from 2025-05-30 to 2025-06-06\n",
      "   ‚úÖ Found 32 articles\n",
      "üîç Searching from 2025-06-06 to 2025-06-13\n",
      "   ‚úÖ Found 38 articles\n",
      "\n",
      "‚úÖ Done. Saved 278 full-text articles to CSV.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION ---\n",
    "# pip install gnews newspaper3k pandas\n",
    "\n",
    "from gnews import GNews\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "query = \"USAID Kenya\"  \n",
    "start_date = datetime(2025, 3, 28)\n",
    "end_date = datetime(2025, 6, 13)\n",
    "chunk_days = 7\n",
    "max_results_per_chunk = 50\n",
    "\n",
    "# --- INITIALIZE ---\n",
    "gnews = GNews(language='en', country='KE', max_results=max_results_per_chunk)\n",
    "current_start = start_date\n",
    "all_articles = []\n",
    "\n",
    "# --- DATE RANGE LOOP ---\n",
    "while current_start < end_date:\n",
    "    current_end = min(current_start + timedelta(days=chunk_days), end_date)\n",
    "    gnews.start_date = current_start\n",
    "    gnews.end_date = current_end\n",
    "    print(f\"üîç Searching from {current_start.date()} to {current_end.date()}\")\n",
    "\n",
    "    try:\n",
    "        results = gnews.get_news(query)\n",
    "        print(f\"   ‚úÖ Found {len(results)} articles\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error during search: {e}\")\n",
    "        results = []\n",
    "\n",
    "    # --- EXTRACT FULL TEXT ---\n",
    "    for article in results:\n",
    "        try:\n",
    "            a = Article(article['url'])\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            all_articles.append({\n",
    "                'title': a.title,\n",
    "                'url': article['url'],\n",
    "                'published_date': article['published date'],\n",
    "                'source': article['publisher']['title'],\n",
    "                'text': a.text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to parse {article['url']}: {e}\")\n",
    "\n",
    "    current_start += timedelta(days=chunk_days)\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(all_articles)\n",
    "df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "df.to_csv(\"../data/raw/gnews_usaid_kenya_full.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done. Saved {len(df)} full-text articles to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6661e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f2d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Searching in language: EN\n",
      "üîç EN: Searching from 2025-03-28 to 2025-04-04\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç EN: Searching from 2025-04-04 to 2025-04-11\n",
      "   ‚úÖ Found 14 articles\n",
      "üîç EN: Searching from 2025-04-11 to 2025-04-18\n",
      "   ‚úÖ Found 10 articles\n",
      "üîç EN: Searching from 2025-04-18 to 2025-04-25\n",
      "   ‚úÖ Found 24 articles\n",
      "üîç EN: Searching from 2025-04-25 to 2025-05-02\n",
      "   ‚úÖ Found 36 articles\n",
      "üîç EN: Searching from 2025-05-02 to 2025-05-09\n",
      "   ‚úÖ Found 31 articles\n",
      "üîç EN: Searching from 2025-05-09 to 2025-05-16\n",
      "   ‚úÖ Found 28 articles\n",
      "üîç EN: Searching from 2025-05-16 to 2025-05-23\n",
      "   ‚úÖ Found 30 articles\n",
      "üîç EN: Searching from 2025-05-23 to 2025-05-30\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç EN: Searching from 2025-05-30 to 2025-06-03\n",
      "   ‚úÖ Found 23 articles\n",
      "\n",
      "üåê Searching in language: SW\n",
      "üîç SW: Searching from 2025-03-28 to 2025-04-04\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç SW: Searching from 2025-04-04 to 2025-04-11\n",
      "   ‚úÖ Found 14 articles\n",
      "üîç SW: Searching from 2025-04-11 to 2025-04-18\n",
      "   ‚úÖ Found 10 articles\n",
      "üîç SW: Searching from 2025-04-18 to 2025-04-25\n",
      "   ‚úÖ Found 24 articles\n",
      "üîç SW: Searching from 2025-04-25 to 2025-05-02\n",
      "   ‚úÖ Found 36 articles\n",
      "üîç SW: Searching from 2025-05-02 to 2025-05-09\n",
      "   ‚úÖ Found 31 articles\n",
      "üîç SW: Searching from 2025-05-09 to 2025-05-16\n",
      "   ‚úÖ Found 28 articles\n",
      "üîç SW: Searching from 2025-05-16 to 2025-05-23\n",
      "   ‚úÖ Found 30 articles\n",
      "üîç SW: Searching from 2025-05-23 to 2025-05-30\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç SW: Searching from 2025-05-30 to 2025-06-03\n",
      "   ‚úÖ Found 23 articles\n",
      "\n",
      "‚úÖ Done. Saved 508 full-text articles (EN + SW) to CSV.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION ---\n",
    "# pip install gnews newspaper3k pandas\n",
    "\n",
    "from gnews import GNews\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "query = \"USAID Kenya\"\n",
    "start_date = datetime(2025, 3, 28)\n",
    "end_date = datetime(2025, 6, 3)\n",
    "chunk_days = 7\n",
    "max_results_per_chunk = 50\n",
    "languages = ['en', 'sw']  # English and Swahili\n",
    "\n",
    "# --- STORAGE ---\n",
    "all_articles = []\n",
    "\n",
    "# --- LOOP THROUGH LANGUAGES ---\n",
    "for lang in languages:\n",
    "    print(f\"\\nüåê Searching in language: {lang.upper()}\")\n",
    "    current_start = start_date\n",
    "\n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + timedelta(days=chunk_days), end_date)\n",
    "        \n",
    "        gnews = GNews(language=lang, country='KE', max_results=max_results_per_chunk)\n",
    "        gnews.start_date = current_start\n",
    "        gnews.end_date = current_end\n",
    "        \n",
    "        print(f\"üîç {lang.upper()}: Searching from {current_start.date()} to {current_end.date()}\")\n",
    "\n",
    "        try:\n",
    "            results = gnews.get_news(query)\n",
    "            print(f\"   ‚úÖ Found {len(results)} articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error during search: {e}\")\n",
    "            results = []\n",
    "\n",
    "        for article in results:\n",
    "            try:\n",
    "                a = Article(article['url'])\n",
    "                a.download()\n",
    "                a.parse()\n",
    "                all_articles.append({\n",
    "                    'title': a.title,\n",
    "                    'url': article['url'],\n",
    "                    'published_date': article.get('published date'),\n",
    "                    'source': article['publisher']['title'],\n",
    "                    'language': lang,\n",
    "                    'text': a.text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to parse {article['url']}: {e}\")\n",
    "\n",
    "        current_start += timedelta(days=chunk_days)\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(all_articles)\n",
    "df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "df.to_csv(\"../data/raw/gnews_usaid_kenya_full_en_sw.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done. Saved {len(df)} full-text articles (EN + SW) to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74c2b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 508 entries, 0 to 507\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           508 non-null    object \n",
      " 1   url             508 non-null    object \n",
      " 2   published_date  508 non-null    object \n",
      " 3   source          508 non-null    object \n",
      " 4   language        508 non-null    object \n",
      " 5   text            0 non-null      float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 23.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMimwFBV...</td>\n",
       "      <td>2025-04-04 07:00:00+00:00</td>\n",
       "      <td>Daily Nation</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMirgFBV...</td>\n",
       "      <td>2025-04-01 07:00:00+00:00</td>\n",
       "      <td>Kenyans</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiigFBV...</td>\n",
       "      <td>2025-04-02 07:00:00+00:00</td>\n",
       "      <td>NTV Kenya</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMimwFBV...</td>\n",
       "      <td>2025-03-31 07:00:00+00:00</td>\n",
       "      <td>KBC Digital</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMixwFBV...</td>\n",
       "      <td>2025-04-01 07:00:00+00:00</td>\n",
       "      <td>The EastAfrican</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         title                                                url  \\\n",
       "0  Google News  https://news.google.com/rss/articles/CBMimwFBV...   \n",
       "1  Google News  https://news.google.com/rss/articles/CBMirgFBV...   \n",
       "2  Google News  https://news.google.com/rss/articles/CBMiigFBV...   \n",
       "3  Google News  https://news.google.com/rss/articles/CBMimwFBV...   \n",
       "4  Google News  https://news.google.com/rss/articles/CBMixwFBV...   \n",
       "\n",
       "              published_date           source language  text  \n",
       "0  2025-04-04 07:00:00+00:00     Daily Nation       en   NaN  \n",
       "1  2025-04-01 07:00:00+00:00          Kenyans       en   NaN  \n",
       "2  2025-04-02 07:00:00+00:00        NTV Kenya       en   NaN  \n",
       "3  2025-03-31 07:00:00+00:00      KBC Digital       en   NaN  \n",
       "4  2025-04-01 07:00:00+00:00  The EastAfrican       en   NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"../data/raw/gnews_usaid_kenya_full_en_sw.csv\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab8e24bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Searching in language: EN\n",
      "üîç EN: Searching from 2025-03-28 to 2025-04-04\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç EN: Searching from 2025-04-04 to 2025-04-11\n",
      "   ‚úÖ Found 14 articles\n",
      "üîç EN: Searching from 2025-04-11 to 2025-04-18\n",
      "   ‚úÖ Found 10 articles\n",
      "üîç EN: Searching from 2025-04-18 to 2025-04-25\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç EN: Searching from 2025-04-25 to 2025-05-02\n",
      "   ‚úÖ Found 36 articles\n",
      "üîç EN: Searching from 2025-05-02 to 2025-05-09\n",
      "   ‚úÖ Found 31 articles\n",
      "üîç EN: Searching from 2025-05-09 to 2025-05-16\n",
      "   ‚úÖ Found 28 articles\n",
      "üîç EN: Searching from 2025-05-16 to 2025-05-23\n",
      "   ‚úÖ Found 24 articles\n",
      "üîç EN: Searching from 2025-05-23 to 2025-05-30\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç EN: Searching from 2025-05-30 to 2025-06-03\n",
      "   ‚úÖ Found 21 articles\n",
      "\n",
      "üåê Searching in language: SW\n",
      "üîç SW: Searching from 2025-03-28 to 2025-04-04\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç SW: Searching from 2025-04-04 to 2025-04-11\n",
      "   ‚úÖ Found 15 articles\n",
      "üîç SW: Searching from 2025-04-11 to 2025-04-18\n",
      "   ‚úÖ Found 10 articles\n",
      "üîç SW: Searching from 2025-04-18 to 2025-04-25\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç SW: Searching from 2025-04-25 to 2025-05-02\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç SW: Searching from 2025-05-02 to 2025-05-09\n",
      "   ‚úÖ Found 31 articles\n",
      "üîç SW: Searching from 2025-05-09 to 2025-05-16\n",
      "   ‚úÖ Found 28 articles\n",
      "üîç SW: Searching from 2025-05-16 to 2025-05-23\n",
      "   ‚úÖ Found 29 articles\n",
      "üîç SW: Searching from 2025-05-23 to 2025-05-30\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç SW: Searching from 2025-05-30 to 2025-06-03\n",
      "   ‚úÖ Found 22 articles\n",
      "\n",
      "‚úÖ Done. Saved 496 full-text articles (EN + SW) to CSV.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION ---\n",
    "# pip install gnews newspaper3k pandas requests\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from gnews import GNews\n",
    "from newspaper import Article\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "query = \"USAID Kenya\"\n",
    "start_date = datetime(2025, 3, 28)\n",
    "end_date = datetime(2025, 6, 3)\n",
    "chunk_days = 7\n",
    "max_results_per_chunk = 50\n",
    "languages = ['en', 'sw']\n",
    "\n",
    "# --- STORAGE ---\n",
    "all_articles = []\n",
    "\n",
    "# --- LOOP THROUGH LANGUAGES ---\n",
    "for lang in languages:\n",
    "    print(f\"\\nüåê Searching in language: {lang.upper()}\")\n",
    "    current_start = start_date\n",
    "\n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + timedelta(days=chunk_days), end_date)\n",
    "        \n",
    "        gnews = GNews(language=lang, country='KE', max_results=max_results_per_chunk)\n",
    "        gnews.start_date = current_start\n",
    "        gnews.end_date = current_end\n",
    "        \n",
    "        print(f\"üîç {lang.upper()}: Searching from {current_start.date()} to {current_end.date()}\")\n",
    "\n",
    "        try:\n",
    "            results = gnews.get_news(query)\n",
    "            print(f\"   ‚úÖ Found {len(results)} articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error during search: {e}\")\n",
    "            results = []\n",
    "\n",
    "        for article in results:\n",
    "            try:\n",
    "                # --- Resolve Google News redirect URL to real URL ---\n",
    "                response = requests.get(article['url'], allow_redirects=True, timeout=10)\n",
    "                real_url = response.url\n",
    "\n",
    "                # --- Use real URL to extract content ---\n",
    "                a = Article(real_url)\n",
    "                a.download()\n",
    "                a.parse()\n",
    "\n",
    "                all_articles.append({\n",
    "                    'title': a.title,\n",
    "                    'url': real_url,\n",
    "                    'published_date': article.get('published date'),\n",
    "                    'source': article['publisher']['title'],\n",
    "                    'language': lang,\n",
    "                    'text': a.text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to parse {article['url']} -> {e}\")\n",
    "\n",
    "        current_start += timedelta(days=chunk_days)\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(all_articles)\n",
    "df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "df.to_csv(\"../data/raw/gnews_usaid_kenya_full_en_sw.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done. Saved {len(df)} full-text articles (EN + SW) to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "829a29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Searching in language: EN\n",
      "üîç EN: Searching from 2025-03-28 to 2025-04-04\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç EN: Searching from 2025-04-04 to 2025-04-11\n",
      "   ‚úÖ Found 14 articles\n",
      "üîç EN: Searching from 2025-04-11 to 2025-04-18\n",
      "   ‚úÖ Found 10 articles\n",
      "üîç EN: Searching from 2025-04-18 to 2025-04-25\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç EN: Searching from 2025-04-25 to 2025-05-02\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç EN: Searching from 2025-05-02 to 2025-05-09\n",
      "   ‚úÖ Found 31 articles\n",
      "üîç EN: Searching from 2025-05-09 to 2025-05-16\n",
      "   ‚úÖ Found 28 articles\n",
      "üîç EN: Searching from 2025-05-16 to 2025-05-23\n",
      "   ‚úÖ Found 29 articles\n",
      "üîç EN: Searching from 2025-05-23 to 2025-05-30\n",
      "   ‚úÖ Found 33 articles\n",
      "üîç EN: Searching from 2025-05-30 to 2025-06-03\n",
      "   ‚úÖ Found 22 articles\n",
      "\n",
      "üåê Searching in language: SW\n",
      "üîç SW: Searching from 2025-03-28 to 2025-04-04\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç SW: Searching from 2025-04-04 to 2025-04-11\n",
      "   ‚úÖ Found 14 articles\n",
      "üîç SW: Searching from 2025-04-11 to 2025-04-18\n",
      "   ‚úÖ Found 10 articles\n",
      "üîç SW: Searching from 2025-04-18 to 2025-04-25\n",
      "   ‚úÖ Found 23 articles\n",
      "üîç SW: Searching from 2025-04-25 to 2025-05-02\n",
      "   ‚úÖ Found 35 articles\n",
      "üîç SW: Searching from 2025-05-02 to 2025-05-09\n",
      "   ‚úÖ Found 31 articles\n",
      "üîç SW: Searching from 2025-05-09 to 2025-05-16\n",
      "   ‚úÖ Found 28 articles\n",
      "üîç SW: Searching from 2025-05-16 to 2025-05-23\n",
      "   ‚úÖ Found 29 articles\n",
      "üîç SW: Searching from 2025-05-23 to 2025-05-30\n",
      "   ‚úÖ Found 33 articles\n",
      "üîç SW: Searching from 2025-05-30 to 2025-06-03\n",
      "   ‚úÖ Found 22 articles\n",
      "\n",
      "‚úÖ Done. Saved 496 full-text articles (EN + SW) to CSV.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION ---\n",
    "# pip install gnews newspaper3k pandas requests\n",
    "\n",
    "from gnews import GNews\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "# --- FUNCTION TO RESOLVE REDIRECTS ---\n",
    "def resolve_real_url(google_news_url):\n",
    "    try:\n",
    "        response = requests.get(google_news_url, timeout=10, allow_redirects=True)\n",
    "        return response.url  # Final URL after redirects\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Could not resolve URL {google_news_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "query = \"USAID Kenya\"\n",
    "start_date = datetime(2025, 3, 28)\n",
    "end_date = datetime(2025, 6, 3)\n",
    "chunk_days = 7\n",
    "max_results_per_chunk = 50\n",
    "languages = ['en', 'sw']  # English and Swahili\n",
    "\n",
    "# --- STORAGE ---\n",
    "all_articles = []\n",
    "\n",
    "# --- LOOP THROUGH LANGUAGES ---\n",
    "for lang in languages:\n",
    "    print(f\"\\nüåê Searching in language: {lang.upper()}\")\n",
    "    current_start = start_date\n",
    "\n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + timedelta(days=chunk_days), end_date)\n",
    "\n",
    "        gnews = GNews(language=lang, country='KE', max_results=max_results_per_chunk)\n",
    "        gnews.start_date = current_start\n",
    "        gnews.end_date = current_end\n",
    "\n",
    "        print(f\"üîç {lang.upper()}: Searching from {current_start.date()} to {current_end.date()}\")\n",
    "\n",
    "        try:\n",
    "            results = gnews.get_news(query)\n",
    "            print(f\"   ‚úÖ Found {len(results)} articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error during search: {e}\")\n",
    "            results = []\n",
    "\n",
    "        for article in results:\n",
    "            try:\n",
    "                real_url = resolve_real_url(article['url'])\n",
    "                if real_url is None:\n",
    "                    continue\n",
    "\n",
    "                a = Article(real_url)\n",
    "                a.download()\n",
    "                a.parse()\n",
    "\n",
    "                all_articles.append({\n",
    "                    'title': a.title,\n",
    "                    'url': real_url,\n",
    "                    'published_date': article.get('published date'),\n",
    "                    'source': article['publisher']['title'],\n",
    "                    'language': lang,\n",
    "                    'text': a.text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to parse article from {article['url']}: {e}\")\n",
    "\n",
    "        current_start += timedelta(days=chunk_days)\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(all_articles)\n",
    "df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "df.to_csv(\"../data/raw/gnews_usaid_kenya_full_en_sw.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done. Saved {len(df)} full-text articles (EN + SW) to CSV.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28aeb9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMimwFBV...</td>\n",
       "      <td>2025-04-04 07:00:00+00:00</td>\n",
       "      <td>Daily Nation</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMirgFBV...</td>\n",
       "      <td>2025-04-01 07:00:00+00:00</td>\n",
       "      <td>Kenyans</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiigFBV...</td>\n",
       "      <td>2025-04-02 07:00:00+00:00</td>\n",
       "      <td>NTV Kenya</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMimwFBV...</td>\n",
       "      <td>2025-03-31 07:00:00+00:00</td>\n",
       "      <td>KBC Digital</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMixwFBV...</td>\n",
       "      <td>2025-04-01 07:00:00+00:00</td>\n",
       "      <td>The EastAfrican</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         title                                                url  \\\n",
       "0  Google News  https://news.google.com/rss/articles/CBMimwFBV...   \n",
       "1  Google News  https://news.google.com/rss/articles/CBMirgFBV...   \n",
       "2  Google News  https://news.google.com/rss/articles/CBMiigFBV...   \n",
       "3  Google News  https://news.google.com/rss/articles/CBMimwFBV...   \n",
       "4  Google News  https://news.google.com/rss/articles/CBMixwFBV...   \n",
       "\n",
       "             published_date           source language text  \n",
       "0 2025-04-04 07:00:00+00:00     Daily Nation       en       \n",
       "1 2025-04-01 07:00:00+00:00          Kenyans       en       \n",
       "2 2025-04-02 07:00:00+00:00        NTV Kenya       en       \n",
       "3 2025-03-31 07:00:00+00:00      KBC Digital       en       \n",
       "4 2025-04-01 07:00:00+00:00  The EastAfrican       en       "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459a850",
   "metadata": {},
   "source": [
    "## News from `RSS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c484003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Processing language: EN\n",
      "üîó Reading RSS: https://news.google.com/rss/search?q=USAID Kenya+when:30d&hl=en-KE&gl=KE&ceid=KE:en\n"
     ]
    },
    {
     "ename": "InvalidURL",
     "evalue": "URL can't contain control characters. '/rss/search?q=USAID Kenya+when:30d&hl=en-KE&gl=KE&ceid=KE:en' (found at least ' ')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-11e6966ea1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üîó Reading RSS: {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/feedparser/api.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, response_headers, resolve_relative_uris, sanitize_html)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file_stream_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferrer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         result.update({\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/feedparser/api.py\u001b[0m in \u001b[0;36m_open_resource\u001b[0;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file_stream_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m        \u001b[0;32mand\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file_stream_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ftp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file_stream_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferrer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# try to open with native open function (if url_file_stream_or_string is a filename)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/feedparser/http.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandlers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FeedURLHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# RMK - must clear so we only send our custom User-Agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    543\u001b[0m                                   '_open', req)\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1393\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1394\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1351\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1352\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'skip_accept_encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;31m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mputrequest\u001b[0;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s %s %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_vsn_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_validate_path\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_contains_disallowed_url_pchar_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             raise InvalidURL(f\"URL can't contain control characters. {url!r} \"\n\u001b[0m\u001b[1;32m   1201\u001b[0m                              f\"(found at least {match.group()!r})\")\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidURL\u001b[0m: URL can't contain control characters. '/rss/search?q=USAID Kenya+when:30d&hl=en-KE&gl=KE&ceid=KE:en' (found at least ' ')"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION ---\n",
    "# pip install feedparser newspaper3k pandas\n",
    "\n",
    "import feedparser\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "query = \"USAID Kenya\"\n",
    "rss_feeds = {\n",
    "    'en': [\n",
    "        f\"https://news.google.com/rss/search?q={query}+when:30d&hl=en-KE&gl=KE&ceid=KE:en\"\n",
    "    ],\n",
    "    'sw': [\n",
    "        f\"https://news.google.com/rss/search?q={query}+when:30d&hl=sw&gl=KE&ceid=KE:sw\"\n",
    "    ]\n",
    "}\n",
    "start_date = datetime(2025, 3, 28)\n",
    "end_date = datetime(2025, 6, 3)\n",
    "\n",
    "# --- FETCH + EXTRACT ---\n",
    "articles = []\n",
    "\n",
    "for lang, feeds in rss_feeds.items():\n",
    "    print(f\"\\nüåê Processing language: {lang.upper()}\")\n",
    "    for url in feeds:\n",
    "        print(f\"üîó Reading RSS: {url}\")\n",
    "        feed = feedparser.parse(url)\n",
    "        \n",
    "        for entry in feed.entries:\n",
    "            try:\n",
    "                published = entry.get('published', '') or entry.get('updated', '')\n",
    "                published_dt = pd.to_datetime(published, errors='coerce')\n",
    "                if pd.isna(published_dt):\n",
    "                    continue\n",
    "                if not (start_date <= published_dt <= end_date):\n",
    "                    continue\n",
    "\n",
    "                article = Article(entry.link)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "\n",
    "                articles.append({\n",
    "                    'title': article.title,\n",
    "                    'url': entry.link,\n",
    "                    'published_date': published_dt,\n",
    "                    'source': entry.get('source', {}).get('title', 'Unknown'),\n",
    "                    'language': lang,\n",
    "                    'text': article.text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to parse article: {entry.link} - {e}\")\n",
    "\n",
    "# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(articles)\n",
    "df.to_csv(\"../data/raw/rss_usaid_kenya_en_sw.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Done. Saved {len(df)} full-text articles from RSS feeds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e63aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiowFBV...</td>\n",
       "      <td>2025-04-30 07:00:00+00:00</td>\n",
       "      <td>Tech In Africa</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiZEFVX...</td>\n",
       "      <td>2025-04-21 07:00:00+00:00</td>\n",
       "      <td>TRT Global</td>\n",
       "      <td>sw</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMie0FVX...</td>\n",
       "      <td>2025-06-02 07:00:00+00:00</td>\n",
       "      <td>TechCabal</td>\n",
       "      <td>sw</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMilgFBV...</td>\n",
       "      <td>2025-05-15 07:00:00+00:00</td>\n",
       "      <td>Daily Nation</td>\n",
       "      <td>sw</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiswFBV...</td>\n",
       "      <td>2025-05-22 07:00:00+00:00</td>\n",
       "      <td>Kenyans</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiX0FVX...</td>\n",
       "      <td>2025-06-02 07:00:00+00:00</td>\n",
       "      <td>Centers for Disease Control and Prevention | C...</td>\n",
       "      <td>sw</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiowFBV...</td>\n",
       "      <td>2025-04-22 07:00:00+00:00</td>\n",
       "      <td>Times Higher Education</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMitgFBV...</td>\n",
       "      <td>2025-05-20 07:00:00+00:00</td>\n",
       "      <td>Tuko News</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMi1AFBV...</td>\n",
       "      <td>2025-05-02 07:00:00+00:00</td>\n",
       "      <td>Business Insider Africa</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Google News</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMi1AFBV...</td>\n",
       "      <td>2025-05-02 07:00:00+00:00</td>\n",
       "      <td>Business Insider Africa</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                                                url  \\\n",
       "79   Google News  https://news.google.com/rss/articles/CBMiowFBV...   \n",
       "316  Google News  https://news.google.com/rss/articles/CBMiZEFVX...   \n",
       "485  Google News  https://news.google.com/rss/articles/CBMie0FVX...   \n",
       "396  Google News  https://news.google.com/rss/articles/CBMilgFBV...   \n",
       "167  Google News  https://news.google.com/rss/articles/CBMiswFBV...   \n",
       "493  Google News  https://news.google.com/rss/articles/CBMiX0FVX...   \n",
       "63   Google News  https://news.google.com/rss/articles/CBMiowFBV...   \n",
       "185  Google News  https://news.google.com/rss/articles/CBMitgFBV...   \n",
       "84   Google News  https://news.google.com/rss/articles/CBMi1AFBV...   \n",
       "124  Google News  https://news.google.com/rss/articles/CBMi1AFBV...   \n",
       "\n",
       "               published_date  \\\n",
       "79  2025-04-30 07:00:00+00:00   \n",
       "316 2025-04-21 07:00:00+00:00   \n",
       "485 2025-06-02 07:00:00+00:00   \n",
       "396 2025-05-15 07:00:00+00:00   \n",
       "167 2025-05-22 07:00:00+00:00   \n",
       "493 2025-06-02 07:00:00+00:00   \n",
       "63  2025-04-22 07:00:00+00:00   \n",
       "185 2025-05-20 07:00:00+00:00   \n",
       "84  2025-05-02 07:00:00+00:00   \n",
       "124 2025-05-02 07:00:00+00:00   \n",
       "\n",
       "                                                source language text  \n",
       "79                                      Tech In Africa       en       \n",
       "316                                         TRT Global       sw       \n",
       "485                                          TechCabal       sw       \n",
       "396                                       Daily Nation       sw       \n",
       "167                                            Kenyans       en       \n",
       "493  Centers for Disease Control and Prevention | C...       sw       \n",
       "63                              Times Higher Education       en       \n",
       "185                                          Tuko News       en       \n",
       "84                             Business Insider Africa       en       \n",
       "124                            Business Insider Africa       en       "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd629b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# --- SAVE TO CSV ---\\ndf = pd.DataFrame(all_articles)\\ndf[\\'published_at\\'] = pd.to_datetime(df[\\'published_at\\'])\\ndf.to_csv(\\'data/newsapi_articles.csv\\', index=False)\\n\\nprint(f\"‚úÖ Fetched {len(df)} articles. Saved to data/newsapi_articles.csv.\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# --- SAVE TO CSV ---\n",
    "df = pd.DataFrame(all_articles)\n",
    "df['published_at'] = pd.to_datetime(df['published_at'])\n",
    "df.to_csv('data/newsapi_articles.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Fetched {len(df)} articles. Saved to data/newsapi_articles.csv.\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfb23993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: 426 - {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-05-12, but you have requested 2025-05-04. You may need to upgrade to a paid plan.'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-011d97cb255d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#print(df[['title', 'source', 'published_at']])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   4991\u001b[0m             )\n\u001b[1;32m   4992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4993\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4994\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "api_key = 'bc6c52fd05ee4e63827b7cf45fa0bdb2'\n",
    "\n",
    "# More focused query\n",
    "query = '(USAID OR donor aid OR foreign aid OR healthcare funding) AND Kenya'\n",
    "from_date = '2025-05-04'\n",
    "page_size = 100  # NewsAPI max per page\n",
    "max_pages = 1    # Free tier limit is 100 articles total\n",
    "\n",
    "# Recommended reliable/regional domains (you can expand)\n",
    "domains = 'nation.africa,standardmedia.co.ke,citizen.digital,aljazeera.com,bbc.com,reuters.com,who.int,devex.com,un.org'\n",
    "\n",
    "# --- FETCH ARTICLES ---\n",
    "all_articles = []\n",
    "\n",
    "for page in range(1, max_pages + 1):\n",
    "    url = (\n",
    "        f'https://newsapi.org/v2/everything?'\n",
    "        f'q={query}&'\n",
    "        f'from={from_date}&'\n",
    "        f'sortBy=publishedAt&'\n",
    "        f'domains={domains}&'\n",
    "        f'pageSize={page_size}&'\n",
    "        f'page={page}&'\n",
    "        f'apiKey={api_key}'\n",
    "    )\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Error: {response.status_code} - {response.json()}\")\n",
    "        break\n",
    "\n",
    "    articles = response.json().get('articles', [])\n",
    "    if not articles:\n",
    "        break\n",
    "\n",
    "    for article in articles:\n",
    "        all_articles.append({\n",
    "            'source': article['source']['name'],\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'content': article.get('content'),\n",
    "            'url': article.get('url'),\n",
    "            'published_at': article.get('publishedAt')\n",
    "        })\n",
    "\n",
    "# --- Convert to DataFrame ---\n",
    "df = pd.DataFrame(all_articles)\n",
    "#print(df[['title', 'source', 'published_at']])\n",
    "df.sample(n=10,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1f643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3219a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2 entries, 7 to 9\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   source        2 non-null      object\n",
      " 1   author        2 non-null      object\n",
      " 2   title         2 non-null      object\n",
      " 3   description   2 non-null      object\n",
      " 4   content       2 non-null      object\n",
      " 5   url           2 non-null      object\n",
      " 6   published_at  2 non-null      object\n",
      "dtypes: object(7)\n",
      "memory usage: 128.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_citizen = df[df['source']== 'Citizen.digital']\n",
    "df_citizen.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea13914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 2024, the World Bank projected that the overall unemployment rate of the youth in Kenya was at 5.7 per cent. \\r\\nThe Federation of Kenya (FKE) says that the youth account for over 35 per cent of the‚Ä¶ [+8544 chars]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_citizen['content'].to_list()\n",
    "display( x[0])\n",
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d3061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32491c2",
   "metadata": {},
   "source": [
    "## 4. X Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42bc67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22USAID%20Kenya%20lang%3Aen%20since%3A2024-12-01%20until%3A2025-06-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22USAID%20Kenya%20lang%3Aen%20since%3A2024-12-01%20until%3A2025-06-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n",
      "  0%|          | 0/500 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22USAID%20Kenya%20lang%3Aen%20since%3A2024-12-01%20until%3A2025-06-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7264c2213323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scraping tweets...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msntwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTwitterSearchScraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36mget_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 \u001b[0mpaginationParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'variables'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpaginationVariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPHQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructionsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'search_by_raw_query'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'search_timeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'instructions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graphql_timeline_instructions_to_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'search_by_raw_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'search_timeline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timeline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instructions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Retrieving scroll page {cursor}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapiType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreqParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructionsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstructionsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[0;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mapiType\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPHQL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote_via\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_api_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapiType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapiType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructionsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstructionsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_snscrapeObj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Errors: {\", \".join(errors)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mScraperException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reached unreachable code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22USAID%20Kenya%20lang%3Aen%20since%3A2024-12-01%20until%3A2025-06-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of keywords and common Kenyan locations (can expand)\n",
    "kenyan_locations = ['kenya', 'nairobi', 'mombasa', 'kisumu', 'eldoret', 'nakuru', 'ke']\n",
    "\n",
    "# Function to check if location mentions a Kenyan place\n",
    "def is_kenyan_location(loc):\n",
    "    if not loc:\n",
    "        return False\n",
    "    loc = loc.lower()\n",
    "    return any(place in loc for place in kenyan_locations)\n",
    "\n",
    "# Query Twitter for tweets mentioning USAID and Kenya\n",
    "query = 'USAID Kenya lang:en since:2024-12-01 until:2025-06-01'\n",
    "tweets = []\n",
    "max_results = 500  # increase if needed\n",
    "\n",
    "print(\"Scraping tweets...\")\n",
    "for i, tweet in enumerate(tqdm(sntwitter.TwitterSearchScraper(query).get_items(), total=max_results)):\n",
    "    if i >= max_results:\n",
    "        break\n",
    "    tweets.append({\n",
    "        'date': tweet.date,\n",
    "        'username': tweet.user.username,\n",
    "        'content': tweet.content,\n",
    "        'user_location': tweet.user.location,\n",
    "        'coordinates': tweet.coordinates\n",
    "    })\n",
    "\n",
    "# Load into DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# Filter tweets by user_location (Kenya-only)\n",
    "df['is_kenyan'] = df['user_location'].apply(is_kenyan_location)\n",
    "df_kenya = df[df['is_kenyan'] == True].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTotal tweets scraped: {len(df)}\")\n",
    "print(f\"Tweets with Kenyan location: {len(df_kenya)}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample Kenyan tweet:\")\n",
    "print(df_kenya[['date', 'username', 'content', 'user_location']].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463a5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X API client initialized.\n",
      "\n",
      "--- Attempting to collect X data for 'USAID Kenya lang:en -is:retweet' from 2025-03-01T00:00:00Z ---\n",
      "NOTE: X API free/basic tiers typically limit searches to the last 7 days.\n",
      "Actual search window being attempted: from 2025-06-04T18:32:06Z to 2025-06-11T18:32:06Z\n",
      "Tweepy API Error: 401 Unauthorized\n",
      "Unauthorized\n",
      "This often indicates rate limits, invalid query, or insufficient access permissions (e.g., historical access blocked).\n"
     ]
    }
   ],
   "source": [
    "# src/x_collection.py\n",
    "import os\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "X_BEARER_TOKEN = os.getenv(\"X_BEARER_TOKEN\")\n",
    "\n",
    "try:\n",
    "    client = tweepy.Client(X_BEARER_TOKEN)\n",
    "    print(\"X API client initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing X API client: {e}\")\n",
    "    print(\"Please ensure your X_BEARER_TOKEN is correct and valid. Skipping X data collection.\")\n",
    "    client = None\n",
    "\n",
    "if client:\n",
    "    # Define your keywords and a recent start date (REQUIRED FOR FREE/BASIC TIER)\n",
    "    query_keywords = \"USAID Kenya lang:en -is:retweet\" # English, exclude retweets\n",
    "\n",
    "    # For free tier, search is limited to the last 7 days.\n",
    "    # We will try to set the date to March 1st, 2025, but anticipate failure/no results\n",
    "    # if it's outside the 7-day window for your current execution date.\n",
    "    target_start_date_str = \"2025-03-01T00:00:00Z\" # ISO 8601 format\n",
    "    # The actual 'start_time' for search_recent_tweets must be within 7 days.\n",
    "    # We'll set it to 7 days ago to demonstrate functionality, but this will NOT get March 1st, 2025 data currently.\n",
    "    actual_start_time_for_search = (datetime.utcnow() - timedelta(days=7)).replace(microsecond=0)\n",
    "    end_time_for_search = datetime.utcnow().replace(microsecond=0)\n",
    "\n",
    "    print(f\"\\n--- Attempting to collect X data for '{query_keywords}' from {target_start_date_str} ---\")\n",
    "    print(f\"NOTE: X API free/basic tiers typically limit searches to the last 7 days.\")\n",
    "    print(f\"Actual search window being attempted: from {actual_start_time_for_search.isoformat()}Z to {end_time_for_search.isoformat()}Z\")\n",
    "\n",
    "    tweets_data = []\n",
    "    try:\n",
    "        # max_results is 100 for recent search endpoint on standard access.\n",
    "        # Free tier is even lower.\n",
    "        response = client.search_recent_tweets(\n",
    "            query=query_keywords,\n",
    "            start_time=actual_start_time_for_search,\n",
    "            end_time=end_time_for_search,\n",
    "            tweet_fields=[\"created_at\", \"public_metrics\", \"author_id\", \"lang\"],\n",
    "            expansions=[\"author_id\"],\n",
    "            max_results=100 # Adjust based on your tier and testing\n",
    "        )\n",
    "\n",
    "        if response and response.data:\n",
    "            users = {user[\"id\"]: user for user in response.includes.get(\"users\", [])}\n",
    "            for tweet in response.data:\n",
    "                author_username = users.get(tweet.author_id, {}).get(\"username\", \"[unknown]\")\n",
    "                tweets_data.append({\n",
    "                    \"id\": tweet.id,\n",
    "                    \"text\": tweet.text,\n",
    "                    \"created_at\": tweet.created_at,\n",
    "                    \"retweet_count\": tweet.public_metrics.get(\"retweet_count\", 0),\n",
    "                    \"reply_count\": tweet.public_metrics.get(\"reply_count\", 0),\n",
    "                    \"like_count\": tweet.public_metrics.get(\"like_count\", 0),\n",
    "                    \"quote_count\": tweet.public_metrics.get(\"quote_count\", 0),\n",
    "                    \"author_id\": tweet.author_id,\n",
    "                    \"author_username\": author_username\n",
    "                })\n",
    "            df_tweets = pd.DataFrame(tweets_data)\n",
    "            print(f\"\\n--- X Data Preview (first 5 relevant entries) ---\")\n",
    "            print(df_tweets[['text', 'author_username', 'created_at', 'like_count']].head())\n",
    "            print(f\"Total relevant X tweets collected: {len(df_tweets)}\")\n",
    "            df_tweets.to_csv(\"../data/raw/x_usaid_kenya_tweets.csv\", index=False)\n",
    "            print(\"\\nX data saved to ../data/raw/x_usaid_kenya_tweets.csv\")\n",
    "        else:\n",
    "            print(\"No tweets found for this query within the accessible time range or API limits reached.\")\n",
    "\n",
    "    except tweepy.TweepyException as e:\n",
    "        print(f\"Tweepy API Error: {e}\")\n",
    "        print(\"This often indicates rate limits, invalid query, or insufficient access permissions (e.g., historical access blocked).\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(\"X API client not initialized. Skipping X data collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1ebae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
