{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d118fa4",
   "metadata": {},
   "source": [
    "# **USAID FUNDING SENTIMENTAL ANALYSIS**\n",
    "\n",
    "### **INTRODUCTION**\n",
    "\n",
    "The United States Agency for International Development (USAID) plays a pivotal role in supporting global development through strategic funding initiatives. In recent years, discussions around the withdrawal or reduction of USAID funding have sparked significant public response — both within the United States and in countries that have long relied on this aid.\n",
    "\n",
    "This project seeks to analyze the public sentiment surrounding USAID funding, particularly in relation to controversial political decisions or policy shifts. By leveraging social media data, news articles, and other textual sources, we aim to uncover how these funding changes are perceived by various stakeholders — including American citizens, government officials, development workers, and beneficiaries in aid-receiving countries such as Kenya.\n",
    "\n",
    "Through natural language processing (NLP) techniques, we perform sentiment classification and thematic analysis to better understand the emotional tone, concerns, and priorities reflected in public discourse. The findings from this analysis can offer valuable insights into the social and geopolitical implications of foreign aid decisions and their real-world human impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c682a",
   "metadata": {},
   "source": [
    "### **PROBLEM STATEMENT**\n",
    "\n",
    "In recent years, shifts in U.S. foreign aid policy—particularly under different presidential administrations—have sparked varying public reactions, especially regarding USAID’s role in Kenya and across Africa. While news media and online communities like Reddit reflect public sentiment on these changes, there is limited structured analysis of how people perceive USAID's presence, impact, and funding shifts over time.\n",
    "\n",
    "This lack of insight poses a challenge for stakeholders (e.g., policymakers, development partners, NGOs) who need to understand community perceptions to align strategies, counter misinformation, and strengthen trust. There is, therefore, a need to systematically analyze and compare public sentiment toward USAID across social media and mainstream news platforms.\n",
    "\n",
    "---\n",
    "\n",
    "### **OJECTIVES**\n",
    "\n",
    "1. **Conduct sentiment analysis** using tools like VADER, TextBlob, or spaCy to classify posts and articles as positive, neutral, or negative.\n",
    "\n",
    "2. **Compare sentiment trends** across Reddit and news platforms to identify spikes, policy-driven reactions, or framing differences over time.\n",
    "\n",
    "3. **Visualize and report insights** using graphs, word clouds, and dashboards that highlight the tone and evolution of public opinion on USAID in Kenya.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ca8d0",
   "metadata": {},
   "source": [
    " ## **DATA LOADING AND INSPECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78f89626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_date</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>keyword</th>\n",
       "      <th>search_term</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>muerki</td>\n",
       "      <td>2025-04-15 13:16:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jzrn2...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>The classism I'm seeing in both subs is a good...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Morio_anzenza</td>\n",
       "      <td>2025-04-07 04:21:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jtcvb...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>vindtar</td>\n",
       "      <td>2025-04-05 19:09:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsb14...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>I don't care what good book you read, but it's...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Gold_Smart</td>\n",
       "      <td>2025-03-25 08:18:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jjehw...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>How is kenya prepared to fill the vacuum of US...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>westmaxia</td>\n",
       "      <td>2025-03-08 08:08:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1j6cjz...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1                  Classism in r/Kenya and r/nairobi   \n",
       "2                       EX-USAID people!! Let's talk   \n",
       "3  Why western powers back Israel no matter what ...   \n",
       "4  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                            selftext subreddit         author  \\\n",
       "0  Someone on a different group (different websit...     Kenya         muerki   \n",
       "1  The classism I'm seeing in both subs is a good...     Kenya  Morio_anzenza   \n",
       "2  Are you still in contact with the organisation...     Kenya        vindtar   \n",
       "3  I don't care what good book you read, but it's...     Kenya     Gold_Smart   \n",
       "4  How is kenya prepared to fill the vacuum of US...     Kenya      westmaxia   \n",
       "\n",
       "           created_utc created_date  score  num_comments      keyword  \\\n",
       "0  2025-04-15 13:16:53          NaN    3.0           5.0  usaid kenya   \n",
       "1  2025-04-07 04:21:12          NaN  169.0          95.0  usaid kenya   \n",
       "2  2025-04-05 19:09:10          NaN    2.0           2.0  usaid kenya   \n",
       "3  2025-03-25 08:18:04          NaN   13.0          20.0  usaid kenya   \n",
       "4  2025-03-08 08:08:58          NaN    1.0           6.0  usaid kenya   \n",
       "\n",
       "  search_term date_posted  upvotes  comments  \\\n",
       "0         NaN         NaN      NaN       NaN   \n",
       "1         NaN         NaN      NaN       NaN   \n",
       "2         NaN         NaN      NaN       NaN   \n",
       "3         NaN         NaN      NaN       NaN   \n",
       "4         NaN         NaN      NaN       NaN   \n",
       "\n",
       "                                                 url permalink  \n",
       "0  https://www.reddit.com/r/Kenya/comments/1jzrn2...       NaN  \n",
       "1  https://www.reddit.com/r/Kenya/comments/1jtcvb...       NaN  \n",
       "2  https://www.reddit.com/r/Kenya/comments/1jsb14...       NaN  \n",
       "3  https://www.reddit.com/r/Kenya/comments/1jjehw...       NaN  \n",
       "4  https://www.reddit.com/r/Kenya/comments/1j6cjz...       NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "\n",
    "df1 = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\USAID-Kenya-Sentiment-Analysis\\data\\processed\\reddit_data\\reddit_data.csv\", encoding='utf-8')\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f69cbe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Has DOGE really saved the US government $180bn?</td>\n",
       "      <td>Elon Musk first claimed the department would m...</td>\n",
       "      <td>President Donald Trump and adviser Elon Musk c...</td>\n",
       "      <td>https://www.aljazeera.com/news/2025/6/6/has-do...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Life Story of Ecomobilus Technologies Limi...</td>\n",
       "      <td>By Prof Geoffrey Gitau Here is a story showcas...</td>\n",
       "      <td>By Prof Geoffrey Gitau\\r\\nHere is a story show...</td>\n",
       "      <td>https://cleantechnica.com/2025/05/26/the-life-...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Death, Sexual Violence and Human Trafficking: ...</td>\n",
       "      <td>by Brett Murphy and Anna Maria Barry-Jester \\n...</td>\n",
       "      <td>ProPublica is a nonprofit newsroom that invest...</td>\n",
       "      <td>https://www.propublica.org/article/trump-usaid...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress Should Quickly Approve Trump’s Rescis...</td>\n",
       "      <td>President Donald Trump‘s rescission legislatio...</td>\n",
       "      <td>President Donald Trumps rescission legislation...</td>\n",
       "      <td>https://www.dailysignal.com/2025/06/10/congres...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food Safety Depends On Every Link In The Suppl...</td>\n",
       "      <td>Almost 1 in 10 people globally fall ill from c...</td>\n",
       "      <td>Colorful fish and vegetables can be purchased ...</td>\n",
       "      <td>https://www.forbes.com/sites/daniellenierenber...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    Has DOGE really saved the US government $180bn?   \n",
       "1  The Life Story of Ecomobilus Technologies Limi...   \n",
       "2  Death, Sexual Violence and Human Trafficking: ...   \n",
       "3  Congress Should Quickly Approve Trump’s Rescis...   \n",
       "4  Food Safety Depends On Every Link In The Suppl...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Elon Musk first claimed the department would m...   \n",
       "1  By Prof Geoffrey Gitau Here is a story showcas...   \n",
       "2  by Brett Murphy and Anna Maria Barry-Jester \\n...   \n",
       "3  President Donald Trump‘s rescission legislatio...   \n",
       "4  Almost 1 in 10 people globally fall ill from c...   \n",
       "\n",
       "                                                text  \\\n",
       "0  President Donald Trump and adviser Elon Musk c...   \n",
       "1  By Prof Geoffrey Gitau\\r\\nHere is a story show...   \n",
       "2  ProPublica is a nonprofit newsroom that invest...   \n",
       "3  President Donald Trumps rescission legislation...   \n",
       "4  Colorful fish and vegetables can be purchased ...   \n",
       "\n",
       "                                                 url      keyword  \\\n",
       "0  https://www.aljazeera.com/news/2025/6/6/has-do...  usaid kenya   \n",
       "1  https://cleantechnica.com/2025/05/26/the-life-...  usaid kenya   \n",
       "2  https://www.propublica.org/article/trump-usaid...  usaid kenya   \n",
       "3  https://www.dailysignal.com/2025/06/10/congres...  usaid kenya   \n",
       "4  https://www.forbes.com/sites/daniellenierenber...  usaid kenya   \n",
       "\n",
       "  published_date      source_file  \n",
       "0     2025-06-06  Agatha_news.csv  \n",
       "1     2025-05-26  Agatha_news.csv  \n",
       "2     2025-05-28  Agatha_news.csv  \n",
       "3     2025-06-10  Agatha_news.csv  \n",
       "4     2025-06-06  Agatha_news.csv  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\Documents\\USAID-Kenya-Sentiment-Analysis\\data\\processed\\news_data\\news_data.csv\", encoding='utf-8')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821c818",
   "metadata": {},
   "source": [
    "## **DATA CLEANING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b4c68",
   "metadata": {},
   "source": [
    "**Basic overview of the two data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36b9fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Data Shape: (1289, 15)\n",
      "Reddit Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1289 entries, 0 to 1288\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         1289 non-null   object \n",
      " 1   selftext      901 non-null    object \n",
      " 2   subreddit     1289 non-null   object \n",
      " 3   author        466 non-null    object \n",
      " 4   created_utc   1013 non-null   object \n",
      " 5   created_date  150 non-null    object \n",
      " 6   score         1013 non-null   float64\n",
      " 7   num_comments  833 non-null    float64\n",
      " 8   keyword       742 non-null    object \n",
      " 9   search_term   150 non-null    object \n",
      " 10  date_posted   276 non-null    object \n",
      " 11  upvotes       276 non-null    float64\n",
      " 12  comments      276 non-null    float64\n",
      " 13  url           1289 non-null   object \n",
      " 14  permalink     426 non-null    object \n",
      "dtypes: float64(4), object(11)\n",
      "memory usage: 151.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Shape and general info of the datasets separately\n",
    "print(\"Reddit Data Shape:\", df1.shape)\n",
    "print(\"Reddit Data Info:\")\n",
    "print(df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf05ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News Data Shape: (2549, 7)\n",
      "News Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2549 entries, 0 to 2548\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           2549 non-null   object\n",
      " 1   description     2533 non-null   object\n",
      " 2   text            2524 non-null   object\n",
      " 3   url             2547 non-null   object\n",
      " 4   keyword         2379 non-null   object\n",
      " 5   published_date  2450 non-null   object\n",
      " 6   source_file     2549 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 139.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNews Data Shape:\", df2.shape)\n",
    "print(\"News Data Info:\")    \n",
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3d4ee",
   "metadata": {},
   "source": [
    "### **Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48a4ebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in Reddit Data:\n",
      "selftext         388\n",
      "author           823\n",
      "created_utc      276\n",
      "created_date    1139\n",
      "score            276\n",
      "num_comments     456\n",
      "keyword          547\n",
      "search_term     1139\n",
      "date_posted     1013\n",
      "upvotes         1013\n",
      "comments        1013\n",
      "permalink        863\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#missing vakues for the reddit data\n",
    "missing_values_reddit = df1.isnull().sum()\n",
    "print(\"\\nMissing Values in Reddit Data:\")\n",
    "print(missing_values_reddit[missing_values_reddit > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6593e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Rows in Reddit Data: 25\n"
     ]
    }
   ],
   "source": [
    "#Checking for duplicate rows in the Reddit data\n",
    "duplicate_rows_reddit = df1.duplicated().sum()\n",
    "print(\"\\nDuplicate Rows in Reddit Data:\", duplicate_rows_reddit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9130cdb",
   "metadata": {},
   "source": [
    "**News Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46d9df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in News Data:\n",
      "description        16\n",
      "text               25\n",
      "url                 2\n",
      "keyword           170\n",
      "published_date     99\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Now let us check the missing values for the news data\n",
    "missing_values_news = df2.isnull().sum()\n",
    "print(\"\\nMissing Values in News Data:\")\n",
    "print(missing_values_news[missing_values_news > 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d9f35fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Rows in News Data: 98\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicate rows in the news data\n",
    "duplicate_rows_news = df2.duplicated().sum()\n",
    "print(\"\\nDuplicate Rows in News Data:\", duplicate_rows_news)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c72056",
   "metadata": {},
   "source": [
    "**Removing duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0338c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates: (1264, 15)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate rows\n",
    "df1_cleaned = df1.drop_duplicates()\n",
    "\n",
    "# Check new shape\n",
    "print(\"Shape after removing duplicates:\", df1_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f37d41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates: (2451, 7)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate rows\n",
    "df2_cleaned = df2.drop_duplicates()\n",
    "\n",
    "# Check new shape\n",
    "print(\"Shape after removing duplicates:\", df2_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03a42f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Shape after full cleaning: (843, 15)\n",
      "title             0\n",
      "selftext        235\n",
      "subreddit         0\n",
      "author          377\n",
      "created_utc       0\n",
      "created_date      0\n",
      "score             0\n",
      "num_comments    177\n",
      "keyword           0\n",
      "search_term       0\n",
      "date_posted       0\n",
      "upvotes           0\n",
      "comments          0\n",
      "url               0\n",
      "permalink         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-2b6ed6f711df>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_cleaned['created_utc'] = pd.to_datetime(df1_cleaned['created_utc'], errors='coerce')\n",
      "<ipython-input-28-2b6ed6f711df>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_cleaned['created_date'] = df1_cleaned['created_utc'].dt.date\n",
      "<ipython-input-28-2b6ed6f711df>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_cleaned['date_posted'] = df1_cleaned['created_date']\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\series.py:4517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "<ipython-input-28-2b6ed6f711df>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_cleaned.dropna(subset=['created_utc'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Regenerate created_date and date_posted if created_utc is available\n",
    "df1_cleaned['created_utc'] = pd.to_datetime(df1_cleaned['created_utc'], errors='coerce')\n",
    "df1_cleaned['created_date'] = df1_cleaned['created_utc'].dt.date\n",
    "df1_cleaned['date_posted'] = df1_cleaned['created_date']\n",
    "\n",
    "# Fill missing values\n",
    "df1_cleaned['keyword'].fillna('Unknown', inplace=True)\n",
    "df1_cleaned['search_term'].fillna('Unknown', inplace=True)\n",
    "df1_cleaned['upvotes'].fillna(0, inplace=True)\n",
    "df1_cleaned['comments'].fillna('', inplace=True)\n",
    "df1_cleaned['permalink'].fillna('', inplace=True)\n",
    "\n",
    "# Drop rows where created_utc is still missing\n",
    "df1_cleaned.dropna(subset=['created_utc'], inplace=True)\n",
    "\n",
    "# Final check\n",
    "print(\"✅ Shape after full cleaning:\", df1_cleaned.shape)\n",
    "print(df1_cleaned.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95819640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_date</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>keyword</th>\n",
       "      <th>search_term</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>muerki</td>\n",
       "      <td>2025-04-15 13:16:53</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jzrn2...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>The classism I'm seeing in both subs is a good...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Morio_anzenza</td>\n",
       "      <td>2025-04-07 04:21:12</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>169.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jtcvb...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>vindtar</td>\n",
       "      <td>2025-04-05 19:09:10</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsb14...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>I don't care what good book you read, but it's...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Gold_Smart</td>\n",
       "      <td>2025-03-25 08:18:04</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jjehw...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>How is kenya prepared to fill the vacuum of US...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>westmaxia</td>\n",
       "      <td>2025-03-08 08:08:58</td>\n",
       "      <td>2025-03-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2025-03-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1j6cjz...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1                  Classism in r/Kenya and r/nairobi   \n",
       "2                       EX-USAID people!! Let's talk   \n",
       "3  Why western powers back Israel no matter what ...   \n",
       "4  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                            selftext subreddit         author  \\\n",
       "0  Someone on a different group (different websit...     Kenya         muerki   \n",
       "1  The classism I'm seeing in both subs is a good...     Kenya  Morio_anzenza   \n",
       "2  Are you still in contact with the organisation...     Kenya        vindtar   \n",
       "3  I don't care what good book you read, but it's...     Kenya     Gold_Smart   \n",
       "4  How is kenya prepared to fill the vacuum of US...     Kenya      westmaxia   \n",
       "\n",
       "          created_utc created_date  score  num_comments      keyword  \\\n",
       "0 2025-04-15 13:16:53   2025-04-15    3.0           5.0  usaid kenya   \n",
       "1 2025-04-07 04:21:12   2025-04-07  169.0          95.0  usaid kenya   \n",
       "2 2025-04-05 19:09:10   2025-04-05    2.0           2.0  usaid kenya   \n",
       "3 2025-03-25 08:18:04   2025-03-25   13.0          20.0  usaid kenya   \n",
       "4 2025-03-08 08:08:58   2025-03-08    1.0           6.0  usaid kenya   \n",
       "\n",
       "  search_term date_posted  upvotes comments  \\\n",
       "0     Unknown  2025-04-15      0.0            \n",
       "1     Unknown  2025-04-07      0.0            \n",
       "2     Unknown  2025-04-05      0.0            \n",
       "3     Unknown  2025-03-25      0.0            \n",
       "4     Unknown  2025-03-08      0.0            \n",
       "\n",
       "                                                 url permalink  \n",
       "0  https://www.reddit.com/r/Kenya/comments/1jzrn2...            \n",
       "1  https://www.reddit.com/r/Kenya/comments/1jtcvb...            \n",
       "2  https://www.reddit.com/r/Kenya/comments/1jsb14...            \n",
       "3  https://www.reddit.com/r/Kenya/comments/1jjehw...            \n",
       "4  https://www.reddit.com/r/Kenya/comments/1j6cjz...            "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d03a7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned News Data Shape: (2328, 7)\n",
      "\n",
      "Remaining Missing Values in News Data:\n",
      " title              0\n",
      "description       15\n",
      "text               0\n",
      "url                0\n",
      "keyword           47\n",
      "published_date     0\n",
      "source_file        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing 'text' or 'published_date'\n",
    "df2_cleaned = df2.dropna(subset=['text', 'published_date']).copy()\n",
    "\n",
    "# Convert 'published_date' to datetime\n",
    "df2_cleaned['published_date'] = pd.to_datetime(df2_cleaned['published_date'], errors='coerce')\n",
    "\n",
    "# Drop any remaining rows where conversion failed (if any)\n",
    "df2_cleaned = df2_cleaned.dropna(subset=['published_date'])\n",
    "\n",
    "# Drop duplicates\n",
    "df2_cleaned = df2_cleaned.drop_duplicates()\n",
    "\n",
    "# Reset index\n",
    "df2_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check final shape and missing values\n",
    "print(\"Cleaned News Data Shape:\", df2_cleaned.shape)\n",
    "print(\"\\nRemaining Missing Values in News Data:\\n\", df2_cleaned.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dd086fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining Missing Values After Replacement:\n",
      "title             0\n",
      "description       0\n",
      "text              0\n",
      "url               0\n",
      "keyword           0\n",
      "published_date    0\n",
      "source_file       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values with \"Unknown\"\n",
    "df2_cleaned['description'] = df2_cleaned['description'].fillna(\"Unknown\")\n",
    "df2_cleaned['keyword'] = df2_cleaned['keyword'].fillna(\"Unknown\")\n",
    "\n",
    "# Confirm there are no missing values left\n",
    "print(\"\\nRemaining Missing Values After Replacement:\")\n",
    "print(df2_cleaned.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff6c1add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Has DOGE really saved the US government $180bn?</td>\n",
       "      <td>Elon Musk first claimed the department would m...</td>\n",
       "      <td>President Donald Trump and adviser Elon Musk c...</td>\n",
       "      <td>https://www.aljazeera.com/news/2025/6/6/has-do...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Life Story of Ecomobilus Technologies Limi...</td>\n",
       "      <td>By Prof Geoffrey Gitau Here is a story showcas...</td>\n",
       "      <td>By Prof Geoffrey Gitau\\r\\nHere is a story show...</td>\n",
       "      <td>https://cleantechnica.com/2025/05/26/the-life-...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Death, Sexual Violence and Human Trafficking: ...</td>\n",
       "      <td>by Brett Murphy and Anna Maria Barry-Jester \\n...</td>\n",
       "      <td>ProPublica is a nonprofit newsroom that invest...</td>\n",
       "      <td>https://www.propublica.org/article/trump-usaid...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress Should Quickly Approve Trump’s Rescis...</td>\n",
       "      <td>President Donald Trump‘s rescission legislatio...</td>\n",
       "      <td>President Donald Trumps rescission legislation...</td>\n",
       "      <td>https://www.dailysignal.com/2025/06/10/congres...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food Safety Depends On Every Link In The Suppl...</td>\n",
       "      <td>Almost 1 in 10 people globally fall ill from c...</td>\n",
       "      <td>Colorful fish and vegetables can be purchased ...</td>\n",
       "      <td>https://www.forbes.com/sites/daniellenierenber...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    Has DOGE really saved the US government $180bn?   \n",
       "1  The Life Story of Ecomobilus Technologies Limi...   \n",
       "2  Death, Sexual Violence and Human Trafficking: ...   \n",
       "3  Congress Should Quickly Approve Trump’s Rescis...   \n",
       "4  Food Safety Depends On Every Link In The Suppl...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Elon Musk first claimed the department would m...   \n",
       "1  By Prof Geoffrey Gitau Here is a story showcas...   \n",
       "2  by Brett Murphy and Anna Maria Barry-Jester \\n...   \n",
       "3  President Donald Trump‘s rescission legislatio...   \n",
       "4  Almost 1 in 10 people globally fall ill from c...   \n",
       "\n",
       "                                                text  \\\n",
       "0  President Donald Trump and adviser Elon Musk c...   \n",
       "1  By Prof Geoffrey Gitau\\r\\nHere is a story show...   \n",
       "2  ProPublica is a nonprofit newsroom that invest...   \n",
       "3  President Donald Trumps rescission legislation...   \n",
       "4  Colorful fish and vegetables can be purchased ...   \n",
       "\n",
       "                                                 url      keyword  \\\n",
       "0  https://www.aljazeera.com/news/2025/6/6/has-do...  usaid kenya   \n",
       "1  https://cleantechnica.com/2025/05/26/the-life-...  usaid kenya   \n",
       "2  https://www.propublica.org/article/trump-usaid...  usaid kenya   \n",
       "3  https://www.dailysignal.com/2025/06/10/congres...  usaid kenya   \n",
       "4  https://www.forbes.com/sites/daniellenierenber...  usaid kenya   \n",
       "\n",
       "  published_date      source_file  \n",
       "0     2025-06-06  Agatha_news.csv  \n",
       "1     2025-05-26  Agatha_news.csv  \n",
       "2     2025-05-28  Agatha_news.csv  \n",
       "3     2025-06-10  Agatha_news.csv  \n",
       "4     2025-06-06  Agatha_news.csv  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b57ac4",
   "metadata": {},
   "source": [
    "### **Combining the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb277081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'selftext', 'subreddit', 'author', 'created_utc', 'created_date', 'score', 'num_comments', 'keyword', 'search_term', 'date_posted', 'upvotes', 'comments', 'url', 'permalink']\n",
      "['title', 'description', 'text', 'url', 'keyword', 'published_date', 'source_file']\n"
     ]
    }
   ],
   "source": [
    "print(df1_cleaned.columns.tolist())\n",
    "print(df2_cleaned.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fa5ec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>The classism I'm seeing in both subs is a good...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>I don't care what good book you read, but it's...</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>How is kenya prepared to fill the vacuum of US...</td>\n",
       "      <td>2025-03-08</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1                  Classism in r/Kenya and r/nairobi   \n",
       "2                       EX-USAID people!! Let's talk   \n",
       "3  Why western powers back Israel no matter what ...   \n",
       "4  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                             content        date  source  \n",
       "0  Someone on a different group (different websit...  2025-04-15  Reddit  \n",
       "1  The classism I'm seeing in both subs is a good...  2025-04-07  Reddit  \n",
       "2  Are you still in contact with the organisation...  2025-04-05  Reddit  \n",
       "3  I don't care what good book you read, but it's...  2025-03-25  Reddit  \n",
       "4  How is kenya prepared to fill the vacuum of US...  2025-03-08  Reddit  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename important Reddit columns\n",
    "df1_subset = df1_cleaned[['title', 'selftext', 'created_date']].copy()\n",
    "df1_subset.rename(columns={\n",
    "    'title': 'title',\n",
    "    'selftext': 'content',\n",
    "    'created_date': 'date'\n",
    "}, inplace=True)\n",
    "df1_subset['source'] = 'Reddit'\n",
    "\n",
    "# rename important News columns\n",
    "df2_subset = df2_cleaned[['title', 'description', 'published_date']].copy()\n",
    "df2_subset.rename(columns={\n",
    "    'title': 'title',\n",
    "    'description': 'content',\n",
    "    'published_date': 'date'\n",
    "}, inplace=True)\n",
    "df2_subset['source'] = 'News'\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data = pd.concat([df1_subset, df2_subset], ignore_index=True)\n",
    "combined_data.reset_index(drop=True, inplace=True)   \n",
    "combined_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6d0654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3171 entries, 0 to 3170\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    3171 non-null   object\n",
      " 1   content  2936 non-null   object\n",
      " 2   date     3171 non-null   object\n",
      " 3   source   3171 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 99.2+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_data.shape\n",
    "combined_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dc7d143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in Combined Data:\n",
      "content    235\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#missing values and null values\n",
    "missing_values_combined = combined_data.isnull().sum()\n",
    "print(\"\\nMissing Values in Combined Data:\")\n",
    "print(missing_values_combined[missing_values_combined > 0])\n",
    "# --- IGNORE ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f98bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3171 entries, 0 to 3170\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    3171 non-null   object\n",
      " 1   content  3171 non-null   object\n",
      " 2   date     3171 non-null   object\n",
      " 3   source   3171 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 99.2+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_data['content'] = combined_data['content'].fillna('no_content')\n",
    "combined_data['content'] = combined_data['content'].astype(str) \n",
    "\n",
    "combined_data.shape\n",
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b027cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>The classism I'm seeing in both subs is a good...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>I don't care what good book you read, but it's...</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>How is kenya prepared to fill the vacuum of US...</td>\n",
       "      <td>2025-03-08</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1                  Classism in r/Kenya and r/nairobi   \n",
       "2                       EX-USAID people!! Let's talk   \n",
       "3  Why western powers back Israel no matter what ...   \n",
       "4  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                             content        date  source  \n",
       "0  Someone on a different group (different websit...  2025-04-15  Reddit  \n",
       "1  The classism I'm seeing in both subs is a good...  2025-04-07  Reddit  \n",
       "2  Are you still in contact with the organisation...  2025-04-05  Reddit  \n",
       "3  I don't care what good book you read, but it's...  2025-03-25  Reddit  \n",
       "4  How is kenya prepared to fill the vacuum of US...  2025-03-08  Reddit  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have realized the text column in df2 was left out and needs to be combined with the description column\n",
    "\n",
    "df2_cleaned['content'] = (\n",
    "    df2_cleaned['description'].fillna('') + ' ' + df2_cleaned['text'].fillna('')\n",
    ")\n",
    "df2_subset = df2_cleaned[['title', 'content', 'published_date']].copy()\n",
    "df2_subset.rename(columns={'published_date': 'date'}, inplace=True)\n",
    "df2_subset['source'] = 'News'\n",
    "\n",
    "# Then combine\n",
    "combined_data = pd.concat([df1_subset, df2_subset], ignore_index=True)\n",
    "combined_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92275a9",
   "metadata": {},
   "source": [
    "Our data is now properly cleaned and ready for text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e0ca1",
   "metadata": {},
   "source": [
    "## **TEXT PROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4662da",
   "metadata": {},
   "source": [
    "This includes:\n",
    "\n",
    "- Lowercasing\n",
    "\n",
    "- Removing punctuation and digits\n",
    "\n",
    "- Tokenization\n",
    "\n",
    "- Stopword removal\n",
    "\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a509128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Downloading necessary NLTK resources(only once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce3c12e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usaid left month ago arvs kenya</td>\n",
       "      <td>someone different group different website aski...</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classism rkenya rnairobi</td>\n",
       "      <td>classism im seeing sub good example current so...</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exusaid people let talk</td>\n",
       "      <td>still contact organisation benefited usaid ica...</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>western power back israel matter kenya avoid u...</td>\n",
       "      <td>dont care good book read clear israel settler ...</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kenya capable funding need usaid disbanded</td>\n",
       "      <td>kenya prepared fill vacuum usaid assistance</td>\n",
       "      <td>2025-03-08</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                    usaid left month ago arvs kenya   \n",
       "1                           classism rkenya rnairobi   \n",
       "2                            exusaid people let talk   \n",
       "3  western power back israel matter kenya avoid u...   \n",
       "4         kenya capable funding need usaid disbanded   \n",
       "\n",
       "                                             content        date  source  \n",
       "0  someone different group different website aski...  2025-04-15  Reddit  \n",
       "1  classism im seeing sub good example current so...  2025-04-07  Reddit  \n",
       "2  still contact organisation benefited usaid ica...  2025-04-05  Reddit  \n",
       "3  dont care good book read clear israel settler ...  2025-03-25  Reddit  \n",
       "4        kenya prepared fill vacuum usaid assistance  2025-03-08  Reddit  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text processing function setup\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#Apply the preprocessing function to the 'content' column\n",
    "combined_data['title'] = combined_data['title'].apply(preprocess_text)\n",
    "combined_data['content'] = combined_data['content'].apply(preprocess_text)\n",
    "# Check the first few rows after preprocessing\n",
    "combined_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c95ef9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News      2328\n",
       "Reddit     843\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data['source'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330dd64d",
   "metadata": {},
   "source": [
    "## NLP Analysis\n",
    "**Topic Modeling, Clustering & Sentiment Analysis**\n",
    "\n",
    "To gain deep insights from text data, we will follow a structured Natural Language Processing (NLP) workflow consisting of three main components:\n",
    "\n",
    "\n",
    "#### 1. Topic Modeling (Latent Dirichlet Allocation - LDA)\n",
    "\n",
    "**Objective:**  \n",
    "Discover hidden thematic structures (topics) within the text data.\n",
    "  \n",
    "It helps uncover the main subjects discussed across documents without any labels. This allows for summarizing large collections of text and identifying what people are talking about.\n",
    "\n",
    "**Output:**  \n",
    "- A list of topics with associated keywords  \n",
    "- Distribution of topics across documents  \n",
    "\n",
    "####  2. Text Clustering (e.g., K-Means with TF-IDF)\n",
    "\n",
    "**Objective:**  \n",
    "Group similar documents into clusters based on textual similarity.\n",
    "\n",
    "After identifying key topics, clustering helps us understand how similar documents are grouped based on word usage and topic content.\n",
    "\n",
    "**Output:**  \n",
    "- Cluster labels for each document  \n",
    "- Visual representations (e.g., t-SNE, PCA) showing document groupings  \n",
    "\n",
    "####  3. Sentiment Analysis\n",
    "\n",
    "**Objective:**  \n",
    "Analyze the emotional tone (positive, negative, or neutral) of the text.\n",
    "\n",
    "**Output:**  \n",
    "- Sentiment scores or labels  \n",
    "- Sentiment distribution per topic or cluster  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda26b0a",
   "metadata": {},
   "source": [
    "### 1.**Topic Modeling, Clustering & Sentiment Analysis**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
