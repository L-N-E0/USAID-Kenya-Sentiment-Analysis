{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844fe40a",
   "metadata": {},
   "source": [
    "### Merge Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fb297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-1ac5fb5e64e0>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
      "<ipython-input-5-1ac5fb5e64e0>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
      "<ipython-input-5-1ac5fb5e64e0>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_title</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-04-15 13:16:53</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jzrn2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>The classism I'm seeing in both subs is a good...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-04-07 04:21:12</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jtcvb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-04-05 19:09:10</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsb14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>I don't care what good book you read, but it's...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-03-25 08:18:04</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jjehw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>How is kenya prepared to fill the vacuum of US...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-03-08 08:08:58</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1j6cjz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          post_title  \\\n",
       "0  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1                  Classism in r/Kenya and r/nairobi   \n",
       "2                       EX-USAID people!! Let's talk   \n",
       "3  Why western powers back Israel no matter what ...   \n",
       "4  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                                text      keyword  \\\n",
       "0  Someone on a different group (different websit...  usaid kenya   \n",
       "1  The classism I'm seeing in both subs is a good...  usaid kenya   \n",
       "2  Are you still in contact with the organisation...  usaid kenya   \n",
       "3  I don't care what good book you read, but it's...  usaid kenya   \n",
       "4  How is kenya prepared to fill the vacuum of US...  usaid kenya   \n",
       "\n",
       "       published_date                                                url  \n",
       "0 2025-04-15 13:16:53  https://www.reddit.com/r/Kenya/comments/1jzrn2...  \n",
       "1 2025-04-07 04:21:12  https://www.reddit.com/r/Kenya/comments/1jtcvb...  \n",
       "2 2025-04-05 19:09:10  https://www.reddit.com/r/Kenya/comments/1jsb14...  \n",
       "3 2025-03-25 08:18:04  https://www.reddit.com/r/Kenya/comments/1jjehw...  \n",
       "4 2025-03-08 08:08:58  https://www.reddit.com/r/Kenya/comments/1j6cjz...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define paths\n",
    "reddit_raw_path = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\reddit_data\"\n",
    "output_file = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\processed\\Agatha_merged_reddit_dataset.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Define final Reddit column structure\n",
    "final_columns = [\n",
    "    \"post_title\", \"text\", \n",
    "    \"keyword\", \"published_date\", \"url\"\n",
    "]\n",
    "\n",
    "# Load and normalize all Reddit files\n",
    "reddit_files = glob.glob(os.path.join(reddit_raw_path, \"*.csv\"))\n",
    "reddit_dfs = []\n",
    "\n",
    "for file in reddit_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # ==== Rename columns as needed ====\n",
    "    if 'title' in df.columns:\n",
    "        df.rename(columns={'title': 'post_title'}, inplace=True)\n",
    "\n",
    "    # Handle published_date\n",
    "    if 'date_posted' in df.columns:\n",
    "        df['published_date'] = df['date_posted']\n",
    "    elif 'created_utc' in df.columns:\n",
    "        df['published_date'] = df['created_utc']\n",
    "\n",
    "    # Handle text column\n",
    "    if 'text' not in df.columns and 'selftext' in df.columns:\n",
    "        df.rename(columns={'selftext': 'text'}, inplace=True)\n",
    "\n",
    "\n",
    "    # Add missing final columns with None\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Format published_date to datetime\n",
    "    df['published_date'] = df['published_date'].astype(str).str.strip().replace('', pd.NA)\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "\n",
    "    # Keep only final standardized columns\n",
    "    df = df[final_columns]\n",
    "    reddit_dfs.append(df)\n",
    "\n",
    "#  Combine all rows\n",
    "reddit_df = pd.concat(reddit_dfs, ignore_index=True)\n",
    "\n",
    "# Save to the output location\n",
    "reddit_df.to_csv(output_file, index=False)\n",
    "\n",
    "# print the five rows of the dataset\n",
    "reddit_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c57c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1289, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4687bc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post_title          0\n",
       "text              388\n",
       "keyword           547\n",
       "published_date    347\n",
       "url                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the number of missing (NaN) values in each column\n",
    "reddit_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "627b3cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find number of duplicate rows\n",
    "reddit_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c147b01",
   "metadata": {},
   "source": [
    "### Merge News data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cdfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Has DOGE really saved the US government $180bn?</td>\n",
       "      <td>Elon Musk first claimed the department would m...</td>\n",
       "      <td>President Donald Trump and adviser Elon Musk c...</td>\n",
       "      <td>https://www.aljazeera.com/news/2025/6/6/has-do...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06 11:21:51+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Life Story of Ecomobilus Technologies Limi...</td>\n",
       "      <td>By Prof Geoffrey Gitau Here is a story showcas...</td>\n",
       "      <td>By Prof Geoffrey Gitau\\r\\nHere is a story show...</td>\n",
       "      <td>https://cleantechnica.com/2025/05/26/the-life-...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-26 17:13:41+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Death, Sexual Violence and Human Trafficking: ...</td>\n",
       "      <td>by Brett Murphy and Anna Maria Barry-Jester \\n...</td>\n",
       "      <td>ProPublica is a nonprofit newsroom that invest...</td>\n",
       "      <td>https://www.propublica.org/article/trump-usaid...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-28 18:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress Should Quickly Approve Trump’s Rescis...</td>\n",
       "      <td>President Donald Trump‘s rescission legislatio...</td>\n",
       "      <td>President Donald Trumps rescission legislation...</td>\n",
       "      <td>https://www.dailysignal.com/2025/06/10/congres...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-10 12:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food Safety Depends On Every Link In The Suppl...</td>\n",
       "      <td>Almost 1 in 10 people globally fall ill from c...</td>\n",
       "      <td>Colorful fish and vegetables can be purchased ...</td>\n",
       "      <td>https://www.forbes.com/sites/daniellenierenber...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06 13:55:41+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    Has DOGE really saved the US government $180bn?   \n",
       "1  The Life Story of Ecomobilus Technologies Limi...   \n",
       "2  Death, Sexual Violence and Human Trafficking: ...   \n",
       "3  Congress Should Quickly Approve Trump’s Rescis...   \n",
       "4  Food Safety Depends On Every Link In The Suppl...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Elon Musk first claimed the department would m...   \n",
       "1  By Prof Geoffrey Gitau Here is a story showcas...   \n",
       "2  by Brett Murphy and Anna Maria Barry-Jester \\n...   \n",
       "3  President Donald Trump‘s rescission legislatio...   \n",
       "4  Almost 1 in 10 people globally fall ill from c...   \n",
       "\n",
       "                                                text  \\\n",
       "0  President Donald Trump and adviser Elon Musk c...   \n",
       "1  By Prof Geoffrey Gitau\\r\\nHere is a story show...   \n",
       "2  ProPublica is a nonprofit newsroom that invest...   \n",
       "3  President Donald Trumps rescission legislation...   \n",
       "4  Colorful fish and vegetables can be purchased ...   \n",
       "\n",
       "                                                 url      keyword  \\\n",
       "0  https://www.aljazeera.com/news/2025/6/6/has-do...  usaid kenya   \n",
       "1  https://cleantechnica.com/2025/05/26/the-life-...  usaid kenya   \n",
       "2  https://www.propublica.org/article/trump-usaid...  usaid kenya   \n",
       "3  https://www.dailysignal.com/2025/06/10/congres...  usaid kenya   \n",
       "4  https://www.forbes.com/sites/daniellenierenber...  usaid kenya   \n",
       "\n",
       "              published_date  \n",
       "0  2025-06-06 11:21:51+00:00  \n",
       "1  2025-05-26 17:13:41+00:00  \n",
       "2  2025-05-28 18:45:00+00:00  \n",
       "3  2025-06-10 12:00:00+00:00  \n",
       "4  2025-06-06 13:55:41+00:00  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define input and output paths\n",
    "news_raw_path = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\news_data\"\n",
    "output_file = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\processed\\Agatha_merged_news_dataset.csv\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Final column structure based on guidelines\n",
    "final_columns = [\n",
    "    \"title\", \"description\", \"text\", \"url\",\n",
    "    \"keyword\", \"published_date\"\n",
    "]\n",
    "\n",
    "# Process and merge all news data files\n",
    "news_files = glob.glob(os.path.join(news_raw_path, \"*.csv\"))\n",
    "news_dfs = []\n",
    "\n",
    "for file in news_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Rename/transform relevant columns\n",
    "    if 'publishedAt' in df.columns:\n",
    "        df['published_date'] = df['publishedAt']\n",
    "    if 'content' in df.columns:\n",
    "        df['text'] = df['content']\n",
    "\n",
    "\n",
    "    # Fill in missing required columns\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Convert published_date to proper datetime\n",
    "    df['published_date'] = df['published_date'].astype(str).str.strip().replace('', pd.NA)\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "\n",
    "    # Retain only final structure\n",
    "    df = df[final_columns]\n",
    "    news_dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "news_df = pd.concat(news_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final merged file\n",
    "news_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print preview\n",
    "news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bf1cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2549, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf1de30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               0\n",
       "description        16\n",
       "text               25\n",
       "url                 2\n",
       "keyword           170\n",
       "published_date     99\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the number of missing (NaN) values in each column\n",
    "news_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de05ba7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find number of duplicate rows\n",
    "news_df.duplicated().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
