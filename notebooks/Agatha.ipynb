{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844fe40a",
   "metadata": {},
   "source": [
    "### Merge Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fb297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-728bcec0ffc3>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
      "<ipython-input-12-728bcec0ffc3>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
      "<ipython-input-12-728bcec0ffc3>:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>muerki</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-04-15 13:16:53</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jzrn2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>The classism I'm seeing in both subs is a good...</td>\n",
       "      <td>Morio_anzenza</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-04-07 04:21:12</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jtcvb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>vindtar</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-04-05 19:09:10</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsb14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>I don't care what good book you read, but it's...</td>\n",
       "      <td>Gold_Smart</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-03-25 08:18:04</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jjehw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>How is kenya prepared to fill the vacuum of US...</td>\n",
       "      <td>westmaxia</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-03-08 08:08:58</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1j6cjz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source subreddit                                         post_title  \\\n",
       "0  Reddit     Kenya  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1  Reddit     Kenya                  Classism in r/Kenya and r/nairobi   \n",
       "2  Reddit     Kenya                       EX-USAID people!! Let's talk   \n",
       "3  Reddit     Kenya  Why western powers back Israel no matter what ...   \n",
       "4  Reddit     Kenya  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                                text         author  \\\n",
       "0  Someone on a different group (different websit...         muerki   \n",
       "1  The classism I'm seeing in both subs is a good...  Morio_anzenza   \n",
       "2  Are you still in contact with the organisation...        vindtar   \n",
       "3  I don't care what good book you read, but it's...     Gold_Smart   \n",
       "4  How is kenya prepared to fill the vacuum of US...      westmaxia   \n",
       "\n",
       "       keyword      published_date  \\\n",
       "0  usaid kenya 2025-04-15 13:16:53   \n",
       "1  usaid kenya 2025-04-07 04:21:12   \n",
       "2  usaid kenya 2025-04-05 19:09:10   \n",
       "3  usaid kenya 2025-03-25 08:18:04   \n",
       "4  usaid kenya 2025-03-08 08:08:58   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/Kenya/comments/1jzrn2...  \n",
       "1  https://www.reddit.com/r/Kenya/comments/1jtcvb...  \n",
       "2  https://www.reddit.com/r/Kenya/comments/1jsb14...  \n",
       "3  https://www.reddit.com/r/Kenya/comments/1jjehw...  \n",
       "4  https://www.reddit.com/r/Kenya/comments/1j6cjz...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define paths\n",
    "reddit_raw_path = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\reddit_data\"\n",
    "output_file = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\processed\\Agatha_merged_reddit_dataset.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Define final Reddit column structure\n",
    "final_columns = [\n",
    "    \"source\", \"subreddit\", \"post_title\", \"text\", \"author\",\n",
    "    \"keyword\", \"published_date\", \"url\"\n",
    "]\n",
    "\n",
    "# Load and normalize all Reddit files\n",
    "reddit_files = glob.glob(os.path.join(reddit_raw_path, \"*.csv\"))\n",
    "reddit_dfs = []\n",
    "\n",
    "for file in reddit_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # ==== Rename columns as needed ====\n",
    "    if 'title' in df.columns:\n",
    "        df.rename(columns={'title': 'post_title'}, inplace=True)\n",
    "\n",
    "    # Handle published_date\n",
    "    if 'date_posted' in df.columns:\n",
    "        df['published_date'] = df['date_posted']\n",
    "    elif 'created_utc' in df.columns:\n",
    "        df['published_date'] = df['created_utc']\n",
    "\n",
    "    # Handle text column\n",
    "    if 'text' not in df.columns and 'selftext' in df.columns:\n",
    "        df.rename(columns={'selftext': 'text'}, inplace=True)\n",
    "\n",
    "    # Add static source column\n",
    "    df['source'] = 'Reddit'\n",
    "\n",
    "    # Add missing final columns with None\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Format published_date to datetime\n",
    "    df['published_date'] = df['published_date'].astype(str).str.strip().replace('', pd.NA)\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "\n",
    "    # Keep only final standardized columns\n",
    "    df = df[final_columns]\n",
    "    reddit_dfs.append(df)\n",
    "\n",
    "#  Combine all rows\n",
    "reddit_df = pd.concat(reddit_dfs, ignore_index=True)\n",
    "\n",
    "# Save to the output location\n",
    "reddit_df.to_csv(output_file, index=False)\n",
    "\n",
    "# print the five rows of the dataset\n",
    "reddit_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c57c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1293, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4687bc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source              0\n",
       "subreddit           0\n",
       "post_title          0\n",
       "text              389\n",
       "author            827\n",
       "keyword           547\n",
       "published_date    347\n",
       "url                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the number of missing (NaN) values in each column\n",
    "reddit_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c147b01",
   "metadata": {},
   "source": [
    "### Merge News data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cdfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "      <th>language</th>\n",
       "      <th>scraper_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Jazeera English</td>\n",
       "      <td>Has DOGE really saved the US government $180bn?</td>\n",
       "      <td>Elon Musk first claimed the department would m...</td>\n",
       "      <td>President Donald Trump and adviser Elon Musk c...</td>\n",
       "      <td>https://www.aljazeera.com/news/2025/6/6/has-do...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06 11:21:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CleanTechnica</td>\n",
       "      <td>The Life Story of Ecomobilus Technologies Limi...</td>\n",
       "      <td>By Prof Geoffrey Gitau Here is a story showcas...</td>\n",
       "      <td>By Prof Geoffrey Gitau\\r\\nHere is a story show...</td>\n",
       "      <td>https://cleantechnica.com/2025/05/26/the-life-...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-26 17:13:41+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProPublica</td>\n",
       "      <td>Death, Sexual Violence and Human Trafficking: ...</td>\n",
       "      <td>by Brett Murphy and Anna Maria Barry-Jester \\n...</td>\n",
       "      <td>ProPublica is a nonprofit newsroom that invest...</td>\n",
       "      <td>https://www.propublica.org/article/trump-usaid...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-28 18:45:00+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Daily Signal</td>\n",
       "      <td>Congress Should Quickly Approve Trump’s Rescis...</td>\n",
       "      <td>President Donald Trump‘s rescission legislatio...</td>\n",
       "      <td>President Donald Trumps rescission legislation...</td>\n",
       "      <td>https://www.dailysignal.com/2025/06/10/congres...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-10 12:00:00+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forbes</td>\n",
       "      <td>Food Safety Depends On Every Link In The Suppl...</td>\n",
       "      <td>Almost 1 in 10 people globally fall ill from c...</td>\n",
       "      <td>Colorful fish and vegetables can be purchased ...</td>\n",
       "      <td>https://www.forbes.com/sites/daniellenierenber...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06 13:55:41+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                              title  \\\n",
       "0  Al Jazeera English    Has DOGE really saved the US government $180bn?   \n",
       "1       CleanTechnica  The Life Story of Ecomobilus Technologies Limi...   \n",
       "2          ProPublica  Death, Sexual Violence and Human Trafficking: ...   \n",
       "3        Daily Signal  Congress Should Quickly Approve Trump’s Rescis...   \n",
       "4              Forbes  Food Safety Depends On Every Link In The Suppl...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Elon Musk first claimed the department would m...   \n",
       "1  By Prof Geoffrey Gitau Here is a story showcas...   \n",
       "2  by Brett Murphy and Anna Maria Barry-Jester \\n...   \n",
       "3  President Donald Trump‘s rescission legislatio...   \n",
       "4  Almost 1 in 10 people globally fall ill from c...   \n",
       "\n",
       "                                                text  \\\n",
       "0  President Donald Trump and adviser Elon Musk c...   \n",
       "1  By Prof Geoffrey Gitau\\r\\nHere is a story show...   \n",
       "2  ProPublica is a nonprofit newsroom that invest...   \n",
       "3  President Donald Trumps rescission legislation...   \n",
       "4  Colorful fish and vegetables can be purchased ...   \n",
       "\n",
       "                                                 url      keyword  \\\n",
       "0  https://www.aljazeera.com/news/2025/6/6/has-do...  usaid kenya   \n",
       "1  https://cleantechnica.com/2025/05/26/the-life-...  usaid kenya   \n",
       "2  https://www.propublica.org/article/trump-usaid...  usaid kenya   \n",
       "3  https://www.dailysignal.com/2025/06/10/congres...  usaid kenya   \n",
       "4  https://www.forbes.com/sites/daniellenierenber...  usaid kenya   \n",
       "\n",
       "              published_date language   scraper_source  \n",
       "0  2025-06-06 11:21:51+00:00       en  Agatha_news.csv  \n",
       "1  2025-05-26 17:13:41+00:00       en  Agatha_news.csv  \n",
       "2  2025-05-28 18:45:00+00:00       en  Agatha_news.csv  \n",
       "3  2025-06-10 12:00:00+00:00       en  Agatha_news.csv  \n",
       "4  2025-06-06 13:55:41+00:00       en  Agatha_news.csv  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define input and output paths\n",
    "news_raw_path = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\news_data\"\n",
    "output_file = r\"C:\\Users\\user\\Desktop\\USAID-Kenya-Sentiment-Analysis\\USAID-Kenya-Sentiment-Analysis\\data\\processed\\Agatha_merged_news_dataset.csv\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Final column structure based on guidelines\n",
    "final_columns = [\n",
    "    \"source\", \"title\", \"description\", \"text\", \"url\",\n",
    "    \"keyword\", \"published_date\", \"language\", \"scraper_source\"\n",
    "]\n",
    "\n",
    "# Process and merge all news data files\n",
    "news_files = glob.glob(os.path.join(news_raw_path, \"*.csv\"))\n",
    "news_dfs = []\n",
    "\n",
    "for file in news_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Rename/transform relevant columns\n",
    "    if 'publishedAt' in df.columns:\n",
    "        df['published_date'] = df['publishedAt']\n",
    "    if 'content' in df.columns:\n",
    "        df['text'] = df['content']\n",
    "\n",
    "    # Static or derived values\n",
    "    df['language'] = 'en'  # Static value\n",
    "    df['scraper_source'] = os.path.basename(file)  # Name of script file (CSV)\n",
    "\n",
    "    # Fill in missing required columns\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Convert published_date to proper datetime\n",
    "    df['published_date'] = df['published_date'].astype(str).str.strip().replace('', pd.NA)\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "\n",
    "    # Retain only final structure\n",
    "    df = df[final_columns]\n",
    "    news_dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "news_df = pd.concat(news_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final merged file\n",
    "news_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print preview\n",
    "news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bf1cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2780, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf1de30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source               0\n",
       "title                0\n",
       "description        786\n",
       "text              1945\n",
       "url                  4\n",
       "keyword           1043\n",
       "published_date     198\n",
       "language             0\n",
       "scraper_source       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the number of missing (NaN) values in each column\n",
    "news_df.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
