{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c301c5d",
   "metadata": {},
   "source": [
    "#  USAID Sentiment Analysis in Kenya\n",
    "\n",
    "#  1. Business Understanding\n",
    "\n",
    "USAID has long played a major role in Kenya’s development — funding health, education, and governance programs. However, recent shifts in US foreign aid policy, including funding cuts and multiple project phaseouts, have sparked growing conversation and concern.\n",
    "\n",
    "This project focuses on analyzing public and media sentiment **after these cuts or the scaling back of USAID programs**. The goal is to uncover:\n",
    "- Public reaction to USAID’s funding changes\n",
    "- Sentiment trends in both news media and online communities\n",
    "- Common concerns, narratives, or misinformation emerging around USAID\n",
    "\n",
    "These insights can support government and development stakeholders in understanding ground-level perception and refining their outreach or policy communication.\n",
    "\n",
    "---\n",
    "\n",
    "#  2. Data Understanding\n",
    "## 2.1 Data Collection\n",
    "We collected data from two main sources:\n",
    "- **NewsAPI articles** referencing USAID and Kenya \n",
    "- **Reddit posts** from relevant subreddits discussing USAID-related topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11201524",
   "metadata": {},
   "source": [
    "### 2.1.1 News Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Agatha_news.csv\n",
      "   → Rows: 592, Columns: 8\n",
      "   → Columns: ['keyword', 'source', 'author', 'title', 'description', 'content', 'publishedAt', 'url']\n",
      "\n",
      " cecilia.newsapi.csv\n",
      "   → Rows: 1145, Columns: 6\n",
      "   → Columns: ['keyword', 'source', 'title', 'description', 'url', 'publishedAt']\n",
      "\n",
      " leo_newsapi_articles_enriched.csv\n",
      "   → Rows: 99, Columns: 8\n",
      "   → Columns: ['source', 'author', 'title', 'description', 'content', 'url', 'published_at', 'full_text']\n",
      "\n",
      " Mbego_news_usaid_kenya_fulltext.csv\n",
      "   → Rows: 24, Columns: 8\n",
      "   → Columns: ['source', 'author', 'title', 'description', 'url', 'publishedAt', 'summary', 'full_text']\n",
      "\n",
      " Mbego_news_usaid_kenya_recent.csv\n",
      "   → Rows: 27, Columns: 7\n",
      "   → Columns: ['source', 'author', 'title', 'description', 'url', 'publishedAt', 'content']\n",
      "\n",
      " ruth_news.csv\n",
      "   → Rows: 20, Columns: 7\n",
      "   → Columns: ['Unnamed: 0', 'source', 'title', 'description', 'content', 'url', 'publishedAt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Files of interest\n",
    "filenames = [\n",
    "    \"Agatha_news.csv\",\n",
    "    \"cecilia.newsapi.csv\",\n",
    "    \"leo_newsapi_articles_enriched.csv\",\n",
    "    \"Mbego_news_usaid_kenya_fulltext.csv\",\n",
    "    \"Mbego_news_usaid_kenya_recent.csv\",\n",
    "    \"ruth_news.csv\"\n",
    "]\n",
    "\n",
    "# Load and display summary\n",
    "news_dfs = {}\n",
    "for file in filenames:\n",
    "    path = \"../data/raw/news_data/\"+file\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        news_dfs[file] = df\n",
    "        print(f\" {file}\")\n",
    "        print(f\"   - Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "        print(f\"   - Columns: {list(df.columns)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaafe7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated and merged News dataset saved with shape: (501, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- FILES TO MERGE (Only files with full_text or complete text for now) ---\n",
    "filenames = [\n",
    "    \"Agatha_news.csv\",\n",
    "    \"leo_newsapi_articles_enriched.csv\",\n",
    "    \"Mbego_news_usaid_kenya_fulltext.csv\",\n",
    "    \"Mbego_news_usaid_kenya_recent.csv\"\n",
    "]\n",
    "\n",
    "# --- FINAL COLUMNS TO KEEP ---\n",
    "final_columns = ['source', 'title', 'description', 'text', 'url', 'keyword', 'published_date']\n",
    "\n",
    "# --- STORAGE FOR CLEANED DFs ---\n",
    "merged_dfs = []\n",
    "\n",
    "for file in filenames:\n",
    "    path = \"../data/raw/news_data/\" + file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Drop unnamed index columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "    # Preserve both full_text and content temporarily\n",
    "    full_text_col = df.columns[df.columns.str.lower() == 'full_text']\n",
    "    content_col = df.columns[df.columns.str.lower() == 'content']\n",
    "\n",
    "    # Assign priority: full_text > content > None\n",
    "    if len(full_text_col) > 0:\n",
    "        df['text'] = df[full_text_col[0]]\n",
    "    elif len(content_col) > 0:\n",
    "        df['text'] = df[content_col[0]]\n",
    "    else:\n",
    "        df['text'] = None\n",
    "\n",
    "    # Standardize other columns\n",
    "    df = df.rename(columns={\n",
    "        'publishedAt': 'published_date',\n",
    "        'published_at': 'published_date'\n",
    "    })\n",
    "\n",
    "    # Add missing columns\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Filter only final columns\n",
    "    df = df[final_columns]\n",
    "\n",
    "    # Drop rows with empty or missing text\n",
    "    df = df[df['text'].notna() & (df['text'].str.strip() != \"\")]\n",
    "\n",
    "    # Fill keyword if missing\n",
    "    df['keyword'] = df['keyword'].fillna(\"Unknown\")\n",
    "\n",
    "    # Convert dates\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "\n",
    "    # Drop invalid rows (no title or url)\n",
    "    df = df.dropna(subset=['url', 'title'])\n",
    "\n",
    "    merged_dfs.append(df)\n",
    "\n",
    "# --- MERGE AND SAVE ---\n",
    "combined_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)\n",
    "\n",
    "combined_df.to_csv(\"../data/processed/Leo_merged_news_dataset.csv\", index=False)\n",
    "print(f\"Deduplicated and merged News dataset saved with shape: {combined_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22043b01",
   "metadata": {},
   "source": [
    "### 2.1.2 Reddit Data Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832c94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agatha_reddit.csv\n",
      "   - Rows: 466, Columns: 9\n",
      "   - Columns: ['title', 'selftext', 'subreddit', 'author', 'created_utc', 'url', 'score', 'num_comments', 'keyword']\n",
      "\n",
      "cecilia.reddit_nbo_ke_africa.csv\n",
      "   - Rows: 29, Columns: 9\n",
      "   - Columns: ['subreddit', 'keyword', 'title', 'text', 'date_posted', 'upvotes', 'comments', 'url', 'permalink']\n",
      "\n",
      "cecilia.redditsubs.csv\n",
      "   - Rows: 247, Columns: 9\n",
      "   - Columns: ['subreddit', 'keyword', 'title', 'text', 'date_posted', 'upvotes', 'comments', 'url', 'permalink']\n",
      "\n",
      "leo_reddit_posts.csv\n",
      "   - Rows: 150, Columns: 10\n",
      "   - Columns: ['subreddit', 'search_term', 'title', 'text', 'created_utc', 'created_date', 'score', 'num_comments', 'permalink', 'url']\n",
      "\n",
      "Mbego_reddit_usaid_kenya.csv\n",
      "   - Rows: 17, Columns: 6\n",
      "   - Columns: ['title', 'score', 'url', 'created', 'subreddit', 'selftext']\n",
      "\n",
      "Mbego_reddit_usaid_kenya2.csv\n",
      "   - Rows: 163, Columns: 6\n",
      "   - Columns: ['title', 'score', 'url', 'created', 'subreddit', 'selftext']\n",
      "\n",
      "reddit_usaid_sentiment.csv\n",
      "   - Rows: 17, Columns: 7\n",
      "   - Columns: ['subreddit', 'title', 'score', 'url', 'created_utc', 'num_comments', 'selftext']\n",
      "\n",
      "ruth_reddit.csv\n",
      "   - Rows: 200, Columns: 8\n",
      "   - Columns: ['Unnamed: 0', 'subreddit', 'title', 'score', 'url', 'created_utc', 'num_comments', 'selftext']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- FILES OF INTEREST ---\n",
    "filenames = [\n",
    "    \"Agatha_reddit.csv\",\n",
    "    \"cecilia.reddit_nbo_ke_africa.csv\",\n",
    "    \"cecilia.redditsubs.csv\",\n",
    "    \"leo_reddit_posts.csv\",\n",
    "    \"Mbego_reddit_usaid_kenya.csv\",\n",
    "    \"Mbego_reddit_usaid_kenya2.csv\",\n",
    "    \"reddit_usaid_sentiment.csv\",\n",
    "    \"ruth_reddit.csv\"\n",
    "]\n",
    "\n",
    "# --- LOAD AND DISPLAY SUMMARY ---\n",
    "reddit_dfs = {}\n",
    "for file in filenames:\n",
    "    path = \"../data/raw/reddit_data/\" + file\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        reddit_dfs[file] = df\n",
    "        print(f\"{file}\")\n",
    "        print(f\"   - Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "        print(f\"   - Columns: {list(df.columns)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18c1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated and merged Reddit dataset saved with shape: (839, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- FILES TO MERGE ---\n",
    "filenames = [\n",
    "    \"Agatha_reddit.csv\",\n",
    "    \"cecilia.reddit_nbo_ke_africa.csv\",\n",
    "    \"cecilia.redditsubs.csv\",\n",
    "    \"leo_reddit_posts.csv\",\n",
    "    \"Mbego_reddit_usaid_kenya.csv\",\n",
    "    \"Mbego_reddit_usaid_kenya2.csv\",\n",
    "    \"reddit_usaid_sentiment.csv\",\n",
    "    \"ruth_reddit.csv\"\n",
    "]\n",
    "\n",
    "# --- FINAL COLUMNS ---\n",
    "final_columns = ['subreddit', 'title', 'text', 'url', 'created_date', 'keyword']\n",
    "merged_dfs = []\n",
    "\n",
    "for file in filenames:\n",
    "    path = \"../data/raw/reddit_data/\" + file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Remove unnamed index if present\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "    # Rename relevant columns\n",
    "    df = df.rename(columns={\n",
    "        'selftext': 'text',\n",
    "        'search_term': 'keyword',\n",
    "        'date_posted': 'created_date',\n",
    "        'created': 'created_date'\n",
    "    })\n",
    "\n",
    "    # If 'created_utc' exists, convert from Unix timestamp to datetime\n",
    "    if 'created_utc' in df.columns:\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s', errors='coerce')\n",
    "        df['created_date'] = df['created_utc']\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Subset to relevant columns\n",
    "    df = df[final_columns]\n",
    "\n",
    "    # Fill missing keyword\n",
    "    df['keyword'] = df['keyword'].fillna(\"Unknown\")\n",
    "\n",
    "    # Convert created_date column to datetime\n",
    "    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
    "\n",
    "    # Drop rows with no title or url\n",
    "    df = df.dropna(subset=['title', 'url'])\n",
    "\n",
    "    merged_dfs.append(df)\n",
    "\n",
    "# --- MERGE AND DEDUPE ---\n",
    "combined_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)\n",
    "\n",
    "# --- SAVE ---\n",
    "combined_df.to_csv(\"../data/processed/Leo_merged_reddit_dataset.csv\", index=False)\n",
    "print(f\"Deduplicated and merged Reddit dataset saved with shape: {combined_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f557d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
