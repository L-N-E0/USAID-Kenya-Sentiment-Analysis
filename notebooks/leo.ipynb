{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c301c5d",
   "metadata": {},
   "source": [
    "#  USAID Sentiment Analysis in Kenya\n",
    "\n",
    "#  1. Business Understanding\n",
    "\n",
    "USAID has long played a major role in Kenya’s development — funding health, education, and governance programs. However, recent shifts in US foreign aid policy, including funding cuts and multiple project phaseouts, have sparked growing conversation and concern.\n",
    "\n",
    "This project focuses on analyzing public and media sentiment **after these cuts or the scaling back of USAID programs**. The goal is to uncover:\n",
    "- Public reaction to USAID’s funding changes\n",
    "- Sentiment trends in both news media and online communities\n",
    "- Common concerns, narratives, or misinformation emerging around USAID\n",
    "\n",
    "These insights can support government and development stakeholders in understanding ground-level perception and refining their outreach or policy communication.\n",
    "\n",
    "---\n",
    "\n",
    "#  2. Data Understanding\n",
    "## 2.1 Data Collection\n",
    "We collected data from two main sources:\n",
    "- **NewsAPI articles** referencing USAID and Kenya \n",
    "- **Reddit posts** from relevant subreddits discussing USAID-related topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11201524",
   "metadata": {},
   "source": [
    "### 2.1.1 News Data Collection\n",
    "\n",
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5292b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agatha_news_fulltext.csv\n",
      "   - Rows: 562, Columns: 8\n",
      "   - Columns: ['keyword', 'source', 'author', 'title', 'publishedAt', 'summary', 'text', 'url']\n",
      "\n",
      "newsapi_usaid_articles.csv\n",
      "   - Rows: 89, Columns: 6\n",
      "   - Columns: ['title', 'description', 'url', 'publishedAt', 'source', 'content']\n",
      "\n",
      "leo_newsapi_articles_enriched.csv\n",
      "   - Rows: 99, Columns: 8\n",
      "   - Columns: ['source', 'author', 'title', 'description', 'content', 'url', 'published_at', 'full_text']\n",
      "\n",
      "Mbego_news_usaid_kenya_fulltext.csv\n",
      "   - Rows: 24, Columns: 8\n",
      "   - Columns: ['source', 'author', 'title', 'description', 'url', 'publishedAt', 'summary', 'full_text']\n",
      "\n",
      "Agatha_news.csv\n",
      "   - Rows: 592, Columns: 8\n",
      "   - Columns: ['keyword', 'source', 'author', 'title', 'description', 'content', 'publishedAt', 'url']\n",
      "\n",
      "ruth_news.csv\n",
      "   - Rows: 20, Columns: 7\n",
      "   - Columns: ['Unnamed: 0', 'source', 'title', 'description', 'content', 'url', 'publishedAt']\n",
      "\n",
      "cecilia.newsapi.csv\n",
      "   - Rows: 1787, Columns: 9\n",
      "   - Columns: ['keyword', 'source', 'author', 'title', 'description', 'content', 'url', 'publishedAt', 'urlToImage']\n",
      "\n",
      "Mbego_news_usaid_kenya_recent.csv\n",
      "   - Rows: 27, Columns: 7\n",
      "   - Columns: ['source', 'author', 'title', 'description', 'url', 'publishedAt', 'content']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# --- DIRECTORY PATH ---\n",
    "data_dir = \"../data/raw/news_data/\"\n",
    "\n",
    "# --- GET ALL CSV FILES IN THE DIRECTORY ---\n",
    "csv_files = glob(data_dir + \"*.csv\")\n",
    "\n",
    "# --- LOAD AND DISPLAY SUMMARY ---\n",
    "news_dfs = {}\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        news_dfs[file] = df\n",
    "        print(f\"{file.split('/')[-1]}\")\n",
    "        print(f\"   - Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "        print(f\"   - Columns: {list(df.columns)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd6748",
   "metadata": {},
   "source": [
    "### Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee42eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Agatha_news_fulltext.csv\n",
      "Processing: newsapi_usaid_articles.csv\n",
      "Skipped (no text/full_text found): ../data/raw/news_data/newsapi_usaid_articles.csv\n",
      "Processing: leo_newsapi_articles_enriched.csv\n",
      "Processing: Mbego_news_usaid_kenya_fulltext.csv\n",
      "Processing: Agatha_news.csv\n",
      "Skipped (no text/full_text found): ../data/raw/news_data/Agatha_news.csv\n",
      "Processing: ruth_news.csv\n",
      "Skipped (no text/full_text found): ../data/raw/news_data/ruth_news.csv\n",
      "Processing: cecilia.newsapi.csv\n",
      "Skipped (no text/full_text found): ../data/raw/news_data/cecilia.newsapi.csv\n",
      "Processing: Mbego_news_usaid_kenya_recent.csv\n",
      "Skipped (no text/full_text found): ../data/raw/news_data/Mbego_news_usaid_kenya_recent.csv\n",
      "Merged News dataset saved with shape: (471, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all CSV files in the news_data folder\n",
    "news_files = glob(\"../data/raw/news_data/*.csv\")\n",
    "\n",
    "# Final columns to keep\n",
    "final_columns = ['source', 'title', 'description', 'text', 'url', 'keyword', 'published_date']\n",
    "\n",
    "# List to store clean DataFrames\n",
    "merged_dfs = []\n",
    "\n",
    "for file in news_files:\n",
    "    print(f\"Processing: {file.split('/')[-1]}\")\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Remove unnamed index columns if any\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "    # Determine which text column to use\n",
    "    if 'full_text' in df.columns:\n",
    "        df['text'] = df['full_text']\n",
    "    elif 'text' in df.columns:\n",
    "        pass  # Use existing 'text'\n",
    "    else:\n",
    "        print(f\"Skipped (no text/full_text found): {file}\")\n",
    "        continue\n",
    "\n",
    "    # Drop rows where text is fully missing or blank\n",
    "    df = df[df['text'].notna() & (df['text'].str.strip() != \"\")]\n",
    "\n",
    "    # Rename date columns\n",
    "    df = df.rename(columns={\n",
    "        'publishedAt': 'published_date',\n",
    "        'published_at': 'published_date'\n",
    "    })\n",
    "\n",
    "    # Add missing expected columns with None\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Restrict to only the required final columns\n",
    "    df = df[final_columns]\n",
    "\n",
    "    # Fill missing keywords\n",
    "    df['keyword'] = df['keyword'].fillna(\"Unknown\")\n",
    "\n",
    "    # Convert to datetime\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "\n",
    "    # Drop rows without title or url (minimal metadata)\n",
    "    df = df.dropna(subset=['url', 'title'])\n",
    "\n",
    "    # Add cleaned DataFrame to list\n",
    "    merged_dfs.append(df)\n",
    "\n",
    "# Concatenate and deduplicate\n",
    "combined_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)\n",
    "\n",
    "# Save the merged file\n",
    "combined_df.to_csv(\"../data/processed/Leo_merged_news_dataset.csv\", index=False)\n",
    "print(f\"Merged News dataset saved with shape: {combined_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22043b01",
   "metadata": {},
   "source": [
    "### 2.1.2 Reddit Data Collection\n",
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "832c94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_usaid_sentiment.csv\n",
      "   - Rows: 17, Columns: 7\n",
      "   - Columns: ['subreddit', 'title', 'score', 'url', 'created_utc', 'num_comments', 'selftext']\n",
      "\n",
      "Mbego_reddit_usaid_kenya2.csv\n",
      "   - Rows: 163, Columns: 6\n",
      "   - Columns: ['title', 'score', 'url', 'created', 'subreddit', 'selftext']\n",
      "\n",
      "Mbego_reddit_usaid_kenya.csv\n",
      "   - Rows: 17, Columns: 6\n",
      "   - Columns: ['title', 'score', 'url', 'created', 'subreddit', 'selftext']\n",
      "\n",
      "cecilia.redditsubs.csv\n",
      "   - Rows: 247, Columns: 9\n",
      "   - Columns: ['subreddit', 'keyword', 'title', 'text', 'date_posted', 'upvotes', 'comments', 'url', 'permalink']\n",
      "\n",
      "leo_reddit_posts.csv\n",
      "   - Rows: 150, Columns: 10\n",
      "   - Columns: ['subreddit', 'search_term', 'title', 'text', 'created_utc', 'created_date', 'score', 'num_comments', 'permalink', 'url']\n",
      "\n",
      "cecilia.reddit_nbo_ke_africa.csv\n",
      "   - Rows: 29, Columns: 9\n",
      "   - Columns: ['subreddit', 'keyword', 'title', 'text', 'date_posted', 'upvotes', 'comments', 'url', 'permalink']\n",
      "\n",
      "reddit_usaid_kenya.csv\n",
      "   - Rows: 17, Columns: 6\n",
      "   - Columns: ['title', 'score', 'url', 'created', 'subreddit', 'selftext']\n",
      "\n",
      "Agatha_reddit.csv\n",
      "   - Rows: 466, Columns: 9\n",
      "   - Columns: ['title', 'selftext', 'subreddit', 'author', 'created_utc', 'url', 'score', 'num_comments', 'keyword']\n",
      "\n",
      "ruth_reddit.csv\n",
      "   - Rows: 200, Columns: 8\n",
      "   - Columns: ['Unnamed: 0', 'subreddit', 'title', 'score', 'url', 'created_utc', 'num_comments', 'selftext']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all Reddit CSVs from folder\n",
    "reddit_files = glob(\"../data/raw/reddit_data/*.csv\")\n",
    "\n",
    "# Display shape and columns of each\n",
    "reddit_dfs = {}\n",
    "for file in reddit_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        reddit_dfs[file] = df\n",
    "        print(f\"{file.split('/')[-1]}\")\n",
    "        print(f\"   - Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "        print(f\"   - Columns: {list(df.columns)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23f557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: reddit_usaid_sentiment.csv\n",
      "Processing: Mbego_reddit_usaid_kenya2.csv\n",
      "Processing: Mbego_reddit_usaid_kenya.csv\n",
      "Processing: cecilia.redditsubs.csv\n",
      "Processing: leo_reddit_posts.csv\n",
      "Processing: cecilia.reddit_nbo_ke_africa.csv\n",
      "Processing: reddit_usaid_kenya.csv\n",
      "Processing: Agatha_reddit.csv\n",
      "Processing: ruth_reddit.csv\n",
      "Merged Reddit dataset saved with shape: (542, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get all Reddit CSV files\n",
    "reddit_files = glob(\"../data/raw/reddit_data/*.csv\")\n",
    "\n",
    "# Final columns to standardize\n",
    "final_columns = ['subreddit', 'title', 'text', 'url', 'created_date', 'keyword']\n",
    "\n",
    "# Store cleaned DataFrames\n",
    "merged_dfs = []\n",
    "\n",
    "for file in reddit_files:\n",
    "    print(f\"Processing: {file.split('/')[-1]}\")\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Drop any unnamed index column\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "    # Normalize relevant columns\n",
    "    df = df.rename(columns={\n",
    "        'selftext': 'text',\n",
    "        'search_term': 'keyword',\n",
    "        'date_posted': 'created_date',\n",
    "        'created': 'created_date'\n",
    "    })\n",
    "\n",
    "    # Handle created_utc if present\n",
    "    if 'created_utc' in df.columns:\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s', errors='coerce')\n",
    "        df['created_date'] = df['created_utc']\n",
    "\n",
    "    # Skip file if neither 'text' nor 'selftext' present\n",
    "    if 'text' not in df.columns or df['text'].isna().all():\n",
    "        print(f\"Skipped (no usable text): {file.split('/')[-1]}\")\n",
    "        continue\n",
    "\n",
    "    # Keep only final columns (fill missing ones with None)\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df = df[final_columns]\n",
    "\n",
    "    # Filter out rows with missing or empty text\n",
    "    df = df[df['text'].notna() & (df['text'].str.strip() != \"\")]\n",
    "\n",
    "    # Fill missing keywords\n",
    "    df['keyword'] = df['keyword'].fillna(\"Unknown\")\n",
    "\n",
    "    # Parse dates safely\n",
    "    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
    "\n",
    "    # Optional: Keep rows even if title or url are missing (for exploratory flexibility)\n",
    "    merged_dfs.append(df)\n",
    "\n",
    "# Merge and deduplicate\n",
    "combined_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "combined_df.to_csv(\"../data/processed/Leo_merged_reddit_dataset.csv\", index=False)\n",
    "print(f\"Merged Reddit dataset saved with shape: {combined_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7c5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b97aed9",
   "metadata": {},
   "source": [
    "## 2.1.3 Unified Data Collection\n",
    "\n",
    "- The group agreed on joint datasets in the `data/processed/news_data` and the `data/processed/reddit_data`subfolders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1377d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = pd.read_csv('../data/processed/reddit_data/reddit_data.csv')\n",
    "news_data = pd.read_csv('../data/processed/news_data/news_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cefe2b",
   "metadata": {},
   "source": [
    "### 2.1.4 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a181710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check data overview\n",
    "def data_overview(df):\n",
    "    print(f\"The dataset has {df.shape[0]} rows and {df.shape[1]} columns\\n\")\n",
    "    display(df.info())\n",
    "    print( \"\\n---Missing Values---\\n\")\n",
    "    display(df.isna().sum())\n",
    "    print( \"\\n---Sample---\\n\")\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46679cdc",
   "metadata": {},
   "source": [
    "### News Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7bb4fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 2549 rows and 7 columns\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2549 entries, 0 to 2548\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           2549 non-null   object\n",
      " 1   description     2533 non-null   object\n",
      " 2   text            2524 non-null   object\n",
      " 3   url             2547 non-null   object\n",
      " 4   keyword         2379 non-null   object\n",
      " 5   published_date  2450 non-null   object\n",
      " 6   source_file     2549 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 139.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Missing Values---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title               0\n",
       "description        16\n",
       "text               25\n",
       "url                 2\n",
       "keyword           170\n",
       "published_date     99\n",
       "source_file         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Sample---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Has DOGE really saved the US government $180bn?</td>\n",
       "      <td>Elon Musk first claimed the department would m...</td>\n",
       "      <td>President Donald Trump and adviser Elon Musk c...</td>\n",
       "      <td>https://www.aljazeera.com/news/2025/6/6/has-do...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Life Story of Ecomobilus Technologies Limi...</td>\n",
       "      <td>By Prof Geoffrey Gitau Here is a story showcas...</td>\n",
       "      <td>By Prof Geoffrey Gitau\\r\\nHere is a story show...</td>\n",
       "      <td>https://cleantechnica.com/2025/05/26/the-life-...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Death, Sexual Violence and Human Trafficking: ...</td>\n",
       "      <td>by Brett Murphy and Anna Maria Barry-Jester \\n...</td>\n",
       "      <td>ProPublica is a nonprofit newsroom that invest...</td>\n",
       "      <td>https://www.propublica.org/article/trump-usaid...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress Should Quickly Approve Trump’s Rescis...</td>\n",
       "      <td>President Donald Trump‘s rescission legislatio...</td>\n",
       "      <td>President Donald Trumps rescission legislation...</td>\n",
       "      <td>https://www.dailysignal.com/2025/06/10/congres...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food Safety Depends On Every Link In The Suppl...</td>\n",
       "      <td>Almost 1 in 10 people globally fall ill from c...</td>\n",
       "      <td>Colorful fish and vegetables can be purchased ...</td>\n",
       "      <td>https://www.forbes.com/sites/daniellenierenber...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>Agatha_news.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    Has DOGE really saved the US government $180bn?   \n",
       "1  The Life Story of Ecomobilus Technologies Limi...   \n",
       "2  Death, Sexual Violence and Human Trafficking: ...   \n",
       "3  Congress Should Quickly Approve Trump’s Rescis...   \n",
       "4  Food Safety Depends On Every Link In The Suppl...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Elon Musk first claimed the department would m...   \n",
       "1  By Prof Geoffrey Gitau Here is a story showcas...   \n",
       "2  by Brett Murphy and Anna Maria Barry-Jester \\n...   \n",
       "3  President Donald Trump‘s rescission legislatio...   \n",
       "4  Almost 1 in 10 people globally fall ill from c...   \n",
       "\n",
       "                                                text  \\\n",
       "0  President Donald Trump and adviser Elon Musk c...   \n",
       "1  By Prof Geoffrey Gitau\\r\\nHere is a story show...   \n",
       "2  ProPublica is a nonprofit newsroom that invest...   \n",
       "3  President Donald Trumps rescission legislation...   \n",
       "4  Colorful fish and vegetables can be purchased ...   \n",
       "\n",
       "                                                 url      keyword  \\\n",
       "0  https://www.aljazeera.com/news/2025/6/6/has-do...  usaid kenya   \n",
       "1  https://cleantechnica.com/2025/05/26/the-life-...  usaid kenya   \n",
       "2  https://www.propublica.org/article/trump-usaid...  usaid kenya   \n",
       "3  https://www.dailysignal.com/2025/06/10/congres...  usaid kenya   \n",
       "4  https://www.forbes.com/sites/daniellenierenber...  usaid kenya   \n",
       "\n",
       "  published_date      source_file  \n",
       "0     2025-06-06  Agatha_news.csv  \n",
       "1     2025-05-26  Agatha_news.csv  \n",
       "2     2025-05-28  Agatha_news.csv  \n",
       "3     2025-06-10  Agatha_news.csv  \n",
       "4     2025-06-06  Agatha_news.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_overview(news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a865a1",
   "metadata": {},
   "source": [
    "### News Dataset Summary\n",
    "\n",
    "The merged news dataset contains **2,549 articles** with 7 columns. Most records have complete `title`, `url`, and `text` fields, which are essential for sentiment analysis. However, there are a few missing values in `description`, `keyword`, and `published_date`.\n",
    "\n",
    "- The `text` column is mostly intact, with only 25 missing entries (less than 1%), making the dataset suitable for text-based analysis.\n",
    "- The `keyword` field is somewhat sparse but can be filled later through data engineering if needed.\n",
    "- The source file column is retained for traceability, in case we need to trace back quality or source bias.\n",
    "\n",
    "This dataset is rich enough for sentiment and thematic analysis on media coverage surrounding USAID in Kenya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ee8bb",
   "metadata": {},
   "source": [
    "### Reddit Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "150600c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1289 rows and 15 columns\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1289 entries, 0 to 1288\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         1289 non-null   object \n",
      " 1   selftext      901 non-null    object \n",
      " 2   subreddit     1289 non-null   object \n",
      " 3   author        466 non-null    object \n",
      " 4   created_utc   1013 non-null   object \n",
      " 5   created_date  150 non-null    object \n",
      " 6   score         1013 non-null   float64\n",
      " 7   num_comments  833 non-null    float64\n",
      " 8   keyword       742 non-null    object \n",
      " 9   search_term   150 non-null    object \n",
      " 10  date_posted   276 non-null    object \n",
      " 11  upvotes       276 non-null    float64\n",
      " 12  comments      276 non-null    float64\n",
      " 13  url           1289 non-null   object \n",
      " 14  permalink     426 non-null    object \n",
      "dtypes: float64(4), object(11)\n",
      "memory usage: 151.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Missing Values---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title              0\n",
       "selftext         388\n",
       "subreddit          0\n",
       "author           823\n",
       "created_utc      276\n",
       "created_date    1139\n",
       "score            276\n",
       "num_comments     456\n",
       "keyword          547\n",
       "search_term     1139\n",
       "date_posted     1013\n",
       "upvotes         1013\n",
       "comments        1013\n",
       "url                0\n",
       "permalink        863\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Sample---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_date</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>keyword</th>\n",
       "      <th>search_term</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>Someone on a different group (different websit...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>muerki</td>\n",
       "      <td>2025-04-15 13:16:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jzrn2...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>The classism I'm seeing in both subs is a good...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Morio_anzenza</td>\n",
       "      <td>2025-04-07 04:21:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jtcvb...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>Are you still in contact with the organisation...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>vindtar</td>\n",
       "      <td>2025-04-05 19:09:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jsb14...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>I don't care what good book you read, but it's...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Gold_Smart</td>\n",
       "      <td>2025-03-25 08:18:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1jjehw...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>How is kenya prepared to fill the vacuum of US...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>westmaxia</td>\n",
       "      <td>2025-03-08 08:08:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/Kenya/comments/1j6cjz...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1                  Classism in r/Kenya and r/nairobi   \n",
       "2                       EX-USAID people!! Let's talk   \n",
       "3  Why western powers back Israel no matter what ...   \n",
       "4  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                            selftext subreddit         author  \\\n",
       "0  Someone on a different group (different websit...     Kenya         muerki   \n",
       "1  The classism I'm seeing in both subs is a good...     Kenya  Morio_anzenza   \n",
       "2  Are you still in contact with the organisation...     Kenya        vindtar   \n",
       "3  I don't care what good book you read, but it's...     Kenya     Gold_Smart   \n",
       "4  How is kenya prepared to fill the vacuum of US...     Kenya      westmaxia   \n",
       "\n",
       "           created_utc created_date  score  num_comments      keyword  \\\n",
       "0  2025-04-15 13:16:53          NaN    3.0           5.0  usaid kenya   \n",
       "1  2025-04-07 04:21:12          NaN  169.0          95.0  usaid kenya   \n",
       "2  2025-04-05 19:09:10          NaN    2.0           2.0  usaid kenya   \n",
       "3  2025-03-25 08:18:04          NaN   13.0          20.0  usaid kenya   \n",
       "4  2025-03-08 08:08:58          NaN    1.0           6.0  usaid kenya   \n",
       "\n",
       "  search_term date_posted  upvotes  comments  \\\n",
       "0         NaN         NaN      NaN       NaN   \n",
       "1         NaN         NaN      NaN       NaN   \n",
       "2         NaN         NaN      NaN       NaN   \n",
       "3         NaN         NaN      NaN       NaN   \n",
       "4         NaN         NaN      NaN       NaN   \n",
       "\n",
       "                                                 url permalink  \n",
       "0  https://www.reddit.com/r/Kenya/comments/1jzrn2...       NaN  \n",
       "1  https://www.reddit.com/r/Kenya/comments/1jtcvb...       NaN  \n",
       "2  https://www.reddit.com/r/Kenya/comments/1jsb14...       NaN  \n",
       "3  https://www.reddit.com/r/Kenya/comments/1jjehw...       NaN  \n",
       "4  https://www.reddit.com/r/Kenya/comments/1j6cjz...       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_overview(reddit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e9ed0",
   "metadata": {},
   "source": [
    "### Reddit Dataset Summary\n",
    "\n",
    "The merged Reddit dataset contains **1,289 posts** and 15 columns. The dataset includes metadata such as `subreddit`, `author`, `score`, and `num_comments`, which can provide context beyond the post content.\n",
    "\n",
    "- The main text content comes from the `selftext` field, which has **388 missing values**, meaning about 70% of posts have usable body content.\n",
    "- Timestamp data is spread across `created_utc`, `created_date`, and `date_posted` with some sparsity — useful for temporal sentiment trends if cleaned carefully.\n",
    "- Several fields like `author`, `keyword`, `search_term`, and engagement metrics (`upvotes`, `comments`) have missing values but can be optionally used depending on the analytical direction.\n",
    "\n",
    "Despite sparsity in some fields, this dataset captures a wide range of public sentiment and discourse related to USAID, especially useful for assessing grassroots reactions after funding changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43dcef",
   "metadata": {},
   "source": [
    "#  3. Data Cleaning\n",
    "- The raw  **news** and **reddit** data shall now be cleaned to a more structured and consitent format before insights could be drawn\n",
    "\n",
    "# 3.1 News Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9aa18a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before data cleaning dropping duplicates ->_(2549, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Shape before data cleaning dropping duplicates ->_{news_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0de8bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1399 rows and 5 columns\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1399 entries, 0 to 2525\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   title           1399 non-null   object        \n",
      " 1   text            1399 non-null   object        \n",
      " 2   keyword         1399 non-null   object        \n",
      " 3   published_date  1325 non-null   datetime64[ns]\n",
      " 4   mentions_kenya  1399 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](1), object(3)\n",
      "memory usage: 56.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Missing Values---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title              0\n",
       "text               0\n",
       "keyword            0\n",
       "published_date    74\n",
       "mentions_kenya     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Sample---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_date</th>\n",
       "      <th>mentions_kenya</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Has DOGE really saved the US government $180bn?</td>\n",
       "      <td>president donald trump and adviser elon musk c...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Life Story of Ecomobilus Technologies Limi...</td>\n",
       "      <td>by prof geoffrey gitau here is a story showcas...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Death, Sexual Violence and Human Trafficking: ...</td>\n",
       "      <td>propublica is a nonprofit newsroom that invest...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress Should Quickly Approve Trump’s Rescis...</td>\n",
       "      <td>president donald trumps rescission legislation...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food Safety Depends On Every Link In The Suppl...</td>\n",
       "      <td>colorful fish and vegetables can be purchased ...</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    Has DOGE really saved the US government $180bn?   \n",
       "1  The Life Story of Ecomobilus Technologies Limi...   \n",
       "2  Death, Sexual Violence and Human Trafficking: ...   \n",
       "3  Congress Should Quickly Approve Trump’s Rescis...   \n",
       "4  Food Safety Depends On Every Link In The Suppl...   \n",
       "\n",
       "                                                text      keyword  \\\n",
       "0  president donald trump and adviser elon musk c...  usaid kenya   \n",
       "1  by prof geoffrey gitau here is a story showcas...  usaid kenya   \n",
       "2  propublica is a nonprofit newsroom that invest...  usaid kenya   \n",
       "3  president donald trumps rescission legislation...  usaid kenya   \n",
       "4  colorful fish and vegetables can be purchased ...  usaid kenya   \n",
       "\n",
       "  published_date  mentions_kenya  \n",
       "0     2025-06-06           False  \n",
       "1     2025-05-26           False  \n",
       "2     2025-05-28           False  \n",
       "3     2025-06-10           False  \n",
       "4     2025-06-06           False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- 1. Drop Duplicates (full and by URL) ---\n",
    "news_data = news_data.drop_duplicates()\n",
    "news_data = news_data.drop_duplicates(subset=['text'])\n",
    "news_data = news_data.drop_duplicates(subset=['url'])\n",
    "\n",
    "# --- 2. Remove Empty or Very Short Posts ---\n",
    "news_data['text'] = news_data['text'].astype(str)\n",
    "news_data = news_data[news_data['text'].str.strip().astype(bool)]\n",
    "news_data = news_data[news_data['text'].str.split().str.len() >= 10]\n",
    "\n",
    "# --- 3. Fix Date Format (don't drop missing dates) ---\n",
    "news_data['published_date'] = pd.to_datetime(news_data['published_date'], errors='coerce')\n",
    "news_data = news_data[~(news_data['published_date'] > pd.Timestamp.now())]  # Drop only future dates\n",
    "\n",
    "# --- 4. Clean and Normalize Text ---\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # URLs\n",
    "    text = re.sub(r\"<.*?>\", '', text)                    # HTML tags\n",
    "    text = re.sub(r\"[@#]\\w+\", '', text)                  # Mentions/hashtags\n",
    "    text = re.sub(r\"[^a-z0-9\\s\\.,!?'\\\"]\", '', text)      # Emojis/symbols\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()             # Whitespace\n",
    "    return text\n",
    "\n",
    "news_data['text'] = news_data['text'].apply(clean_text)\n",
    "\n",
    "# --- 5. Fill Missing Keywords ---\n",
    "news_data['keyword'] = news_data['keyword'].fillna(\"unknown\")\n",
    "\n",
    "# --- 6. Mark if \"kenya\" is mentioned ---\n",
    "news_data['mentions_kenya'] = news_data['text'].str.contains(r'\\bkenya\\b', case=False, na=False)\n",
    "\n",
    "# --- 7. Drop Unnecessary columns ---\n",
    "news_data = news_data.drop(columns= ['description','source_file','url'])\n",
    "\n",
    "# Overview\n",
    "data_overview(news_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9e191",
   "metadata": {},
   "source": [
    "# 3.1 Reddit Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d4e4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows with valid selftext\n",
    "reddit_data = reddit_data[reddit_data['selftext'].notna()]\n",
    "reddit_data = reddit_data[reddit_data['selftext'].str.strip().astype(bool)]\n",
    "reddit_data = reddit_data[reddit_data['selftext'].str.split().str.len() >= 3]\n",
    "\n",
    "# Deduplicate by URL\n",
    "reddit_data = reddit_data.drop_duplicates(subset=['url'])\n",
    "\n",
    "# Date handling\n",
    "reddit_data['created_utc_dt'] = pd.to_datetime(reddit_data['created_utc'], errors='coerce', unit='s')\n",
    "reddit_data['created_date_dt'] = pd.to_datetime(reddit_data['created_date'], errors='coerce')\n",
    "reddit_data['date_posted_dt'] = pd.to_datetime(reddit_data['date_posted'], errors='coerce')\n",
    "\n",
    "reddit_data['created_date'] = (\n",
    "    reddit_data['created_utc_dt']\n",
    "    .fillna(reddit_data['created_date_dt'])\n",
    "    .fillna(reddit_data['date_posted_dt'])\n",
    ").dt.date\n",
    "\n",
    "reddit_data['is_future_date'] = reddit_data['created_date'] > pd.Timestamp.now().date()\n",
    "\n",
    "# Clean text\n",
    "reddit_data['selftext'] = reddit_data['selftext'].apply(clean_text)\n",
    "\n",
    "# Fill keyword\n",
    "reddit_data['keyword'] = reddit_data['keyword'].fillna(\"unknown\")\n",
    "\n",
    "# Rename and drop\n",
    "reddit_data = reddit_data.rename(columns={'selftext': 'text'})\n",
    "cols_to_drop = [\n",
    "    'author', 'created_utc', 'score', 'num_comments', 'search_term',\n",
    "    'date_posted', 'upvotes', 'comments', 'url', 'permalink',\n",
    "    'created_utc_dt', 'created_date_dt', 'date_posted_dt'\n",
    "]\n",
    "reddit_data = reddit_data.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a65bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 538 rows and 6 columns\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 538 entries, 0 to 1288\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           538 non-null    object\n",
      " 1   text            538 non-null    object\n",
      " 2   subreddit       538 non-null    object\n",
      " 3   created_date    123 non-null    object\n",
      " 4   keyword         538 non-null    object\n",
      " 5   is_future_date  538 non-null    bool  \n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 25.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Missing Values---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title               0\n",
       "text                0\n",
       "subreddit           0\n",
       "created_date      415\n",
       "keyword             0\n",
       "is_future_date      0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Sample---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>keyword</th>\n",
       "      <th>is_future_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USAID left a month ago, do we have ARVs in Kenya?</td>\n",
       "      <td>someone on a different group different website...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaT</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classism in r/Kenya and r/nairobi</td>\n",
       "      <td>the classism i'm seeing in both subs is a good...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaT</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EX-USAID people!! Let's talk</td>\n",
       "      <td>are you still in contact with the organisation...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaT</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why western powers back Israel no matter what ...</td>\n",
       "      <td>i don't care what good book you read, but it's...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaT</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is kenya capable of funding its needs now that...</td>\n",
       "      <td>how is kenya prepared to fill the vacuum of us...</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaT</td>\n",
       "      <td>usaid kenya</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  USAID left a month ago, do we have ARVs in Kenya?   \n",
       "1                  Classism in r/Kenya and r/nairobi   \n",
       "2                       EX-USAID people!! Let's talk   \n",
       "3  Why western powers back Israel no matter what ...   \n",
       "4  Is kenya capable of funding its needs now that...   \n",
       "\n",
       "                                                text subreddit created_date  \\\n",
       "0  someone on a different group different website...     Kenya          NaT   \n",
       "1  the classism i'm seeing in both subs is a good...     Kenya          NaT   \n",
       "2  are you still in contact with the organisation...     Kenya          NaT   \n",
       "3  i don't care what good book you read, but it's...     Kenya          NaT   \n",
       "4  how is kenya prepared to fill the vacuum of us...     Kenya          NaT   \n",
       "\n",
       "       keyword  is_future_date  \n",
       "0  usaid kenya           False  \n",
       "1  usaid kenya           False  \n",
       "2  usaid kenya           False  \n",
       "3  usaid kenya           False  \n",
       "4  usaid kenya           False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_overview(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e140b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
