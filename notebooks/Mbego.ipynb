{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0bd178",
   "metadata": {},
   "source": [
    "### Data Collection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e9414",
   "metadata": {},
   "source": [
    "#### 1. Reddit Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d192eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 17 Reddit posts to:\n",
      "   ğŸ“„ CSV:  N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\\reddit_usaid_kenya.csv\n",
      "   ğŸ“„ JSON: N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\\reddit_usaid_kenya.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… Custom output directory\n",
    "output_dir = r\"N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Replace these with your credentials\n",
    "client_id = 'uRoMjHrYqBAUjIFZgJilRg'\n",
    "client_secret = 'dQkm_47qP1qUUzOdvO8piv5AP--L0A'\n",
    "user_agent = 'usaidKenyaScraper by u/IngenuityStunning997'\n",
    "\n",
    "# âœ… Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# âœ… Define search parameters\n",
    "subreddits = ['Kenya', 'development', 'worldnews']\n",
    "query = \"USAID funding OR donor cuts OR USAID health\"\n",
    "limit = 1000\n",
    "\n",
    "# âœ… Collect posts\n",
    "posts = []\n",
    "for subreddit in subreddits:\n",
    "    for submission in reddit.subreddit(subreddit).search(query, limit=limit):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'score': submission.score,\n",
    "            'url': submission.url,\n",
    "            'created': pd.to_datetime(submission.created_utc, unit='s'),\n",
    "            'subreddit': submission.subreddit.display_name,\n",
    "            'selftext': submission.selftext\n",
    "        })\n",
    "\n",
    "# âœ… Convert to DataFrame\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# âœ… Save as CSV and JSON\n",
    "filename = \"reddit_usaid_kenya\"\n",
    "csv_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "json_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "df.to_json(json_path, orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"âœ… Saved {len(df)} Reddit posts to:\")\n",
    "print(f\"   ğŸ“„ CSV:  {csv_path}\")\n",
    "print(f\"   ğŸ“„ JSON: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66baf293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 163 Reddit posts to: output\\reddit_usaid_kenya.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Replace these with your credentials\n",
    "client_id = 'uRoMjHrYqBAUjIFZgJilRg'\n",
    "client_secret = 'dQkm_47qP1qUUzOdvO8piv5AP--L0A'\n",
    "user_agent = 'usaidKenyaScraper by u/IngenuityStunning997'\n",
    "\n",
    "# Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Define your search parameters\n",
    "subreddits = ['Kenya', 'development', 'worldnews']  # Can add more\n",
    "query = \"USAID funding OR donor cuts OR health\"\n",
    "limit = 1000\n",
    "\n",
    "# Fetch posts\n",
    "posts = []\n",
    "for subreddit in subreddits:\n",
    "    for submission in reddit.subreddit(subreddit).search(query, limit=limit):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'score': submission.score,\n",
    "            'url': submission.url,\n",
    "            'created': pd.to_datetime(submission.created_utc, unit='s'),\n",
    "            'subreddit': submission.subreddit.display_name,\n",
    "            'selftext': submission.selftext\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Save to CSV or JSON\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "filename = \"reddit_usaid_kenya\"\n",
    "csv_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… Saved {len(df)} Reddit posts to: {csv_path}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86dd0d",
   "metadata": {},
   "source": [
    "#### 2. NewsAPI.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f26852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“° Saved 27 news articles to: N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\\news_usaid_kenya_recent.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ğŸ“Œ Your NewsAPI key\n",
    "api_key = \"92ec9d796b7d493295eb56be553d8208\"  # Replace with your actual key\n",
    "\n",
    "# âœ… Limit to the last 30 days\n",
    "today = datetime.today()\n",
    "from_date = (today - timedelta(days=29)).strftime('%Y-%m-%d')\n",
    "to_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# ğŸ” Search parameters\n",
    "params = {\n",
    "    'q': 'USAID Kenya',\n",
    "    'from': from_date,\n",
    "    'to': to_date,\n",
    "    'language': 'en',\n",
    "    'sortBy': 'relevancy',\n",
    "    'pageSize': 100\n",
    "}\n",
    "\n",
    "# âœ… Headers to avoid 426 errors\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0',\n",
    "    'X-Api-Key': api_key\n",
    "}\n",
    "\n",
    "# ğŸŒ Endpoint\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# ğŸ“ Output directory\n",
    "output_dir = r\"N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ğŸ“¥ Fetch and process articles\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    articles = response.json().get(\"articles\", [])\n",
    "    news_data = []\n",
    "\n",
    "    for article in articles:\n",
    "        news_data.append({\n",
    "            \"source\": article[\"source\"][\"name\"],\n",
    "            \"author\": article[\"author\"],\n",
    "            \"title\": article[\"title\"],\n",
    "            \"description\": article[\"description\"],\n",
    "            \"url\": article[\"url\"],\n",
    "            \"publishedAt\": article[\"publishedAt\"],\n",
    "            \"content\": article[\"content\"]\n",
    "        })\n",
    "\n",
    "    # Save to DataFrame and CSV\n",
    "    df_news = pd.DataFrame(news_data)\n",
    "    filename = f\"news_usaid_kenya_recent\"\n",
    "    csv_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "    df_news.to_csv(csv_path, index=False)\n",
    "    print(f\"ğŸ“° Saved {len(df_news)} news articles to: {csv_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Failed to fetch articles. Status code: {response.status_code}\")\n",
    "    print(f\"ğŸ’¡ Response content: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d63da9",
   "metadata": {},
   "source": [
    "Combine NewsAPI (to get article URLs) with newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eee95c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Extracting full text for article 1/27\n",
      "ğŸ” Extracting full text for article 2/27\n",
      "ğŸ” Extracting full text for article 3/27\n",
      "ğŸ” Extracting full text for article 4/27\n",
      "ğŸ” Extracting full text for article 5/27\n",
      "ğŸ” Extracting full text for article 6/27\n",
      "ğŸ” Extracting full text for article 7/27\n",
      "ğŸ” Extracting full text for article 8/27\n",
      "ğŸ” Extracting full text for article 9/27\n",
      "âš ï¸ Skipping article due to error: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/sverrealvik/2025/05/20/africa-needs-more-renewables-so-why-is-it-investing-in-fossil-fuels/ on URL https://www.forbes.com/sites/sverrealvik/2025/05/20/africa-needs-more-renewables-so-why-is-it-investing-in-fossil-fuels/\n",
      "ğŸ” Extracting full text for article 10/27\n",
      "ğŸ” Extracting full text for article 11/27\n",
      "ğŸ” Extracting full text for article 12/27\n",
      "ğŸ” Extracting full text for article 13/27\n",
      "ğŸ” Extracting full text for article 14/27\n",
      "ğŸ” Extracting full text for article 15/27\n",
      "âš ï¸ Skipping article due to error: Article `download()` failed with 404 Client Error: Not Found for url: https://abcnews.go.com/Health/wireStory/children-die-usaid-aid-cuts-snap-lifeline-worlds-121863275 on URL https://abcnews.go.com/Health/wireStory/children-die-usaid-aid-cuts-snap-lifeline-worlds-121863275\n",
      "ğŸ” Extracting full text for article 16/27\n",
      "âš ï¸ Skipping article due to error: Article `download()` failed with 403 Client Error: Forbidden for url: https://financialpost.com/pmn/business-pmn/usaids-demise-threatens-a-vital-health-and-climate-solution-in-africa on URL https://financialpost.com/pmn/business-pmn/usaids-demise-threatens-a-vital-health-and-climate-solution-in-africa\n",
      "ğŸ” Extracting full text for article 17/27\n",
      "ğŸ” Extracting full text for article 18/27\n",
      "ğŸ” Extracting full text for article 19/27\n",
      "ğŸ” Extracting full text for article 20/27\n",
      "ğŸ” Extracting full text for article 21/27\n",
      "ğŸ” Extracting full text for article 22/27\n",
      "ğŸ” Extracting full text for article 23/27\n",
      "ğŸ” Extracting full text for article 24/27\n",
      "ğŸ” Extracting full text for article 25/27\n",
      "ğŸ” Extracting full text for article 26/27\n",
      "ğŸ” Extracting full text for article 27/27\n",
      "âœ… Saved 24 articles with full text to: N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\\news_usaid_kenya_fulltext.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "# ğŸ“Œ NewsAPI key\n",
    "api_key = \"92ec9d796b7d493295eb56be553d8208\"  # Replace with your actual key\n",
    "\n",
    "# ğŸ“† Limit dates for Free Plan (last 30 days)\n",
    "today = datetime.today()\n",
    "from_date = (today - timedelta(days=29)).strftime('%Y-%m-%d')\n",
    "to_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# ğŸŒ API query\n",
    "params = {\n",
    "    'q': 'USAID Kenya',\n",
    "    'from': from_date,\n",
    "    'to': to_date,\n",
    "    'language': 'en',\n",
    "    'sortBy': 'relevancy',\n",
    "    'pageSize': 100\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0',\n",
    "    'X-Api-Key': api_key\n",
    "}\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# ğŸ“ Storage path\n",
    "output_dir = r\"N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ğŸ” Fetch article metadata\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    articles = response.json().get(\"articles\", [])\n",
    "    enriched_data = []\n",
    "\n",
    "    for idx, article in enumerate(articles):\n",
    "        print(f\"ğŸ” Extracting full text for article {idx+1}/{len(articles)}\")\n",
    "        article_url = article[\"url\"]\n",
    "\n",
    "        try:\n",
    "            news_article = Article(article_url)\n",
    "            news_article.download()\n",
    "            news_article.parse()\n",
    "\n",
    "            full_text = news_article.text\n",
    "            enriched_data.append({\n",
    "                \"source\": article[\"source\"][\"name\"],\n",
    "                \"author\": article[\"author\"],\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"url\": article_url,\n",
    "                \"publishedAt\": article[\"publishedAt\"],\n",
    "                \"summary\": article[\"content\"],\n",
    "                \"full_text\": full_text\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipping article due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(1)  # Be nice to servers\n",
    "\n",
    "    # ğŸ’¾ Save to CSV\n",
    "    df_full = pd.DataFrame(enriched_data)\n",
    "    csv_path = os.path.join(output_dir, \"news_usaid_kenya_fulltext.csv\")\n",
    "    df_full.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"âœ… Saved {len(df_full)} articles with full text to: {csv_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Failed to fetch articles. Status code: {response.status_code}\")\n",
    "    print(f\"ğŸ’¡ Response: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490cca4",
   "metadata": {},
   "source": [
    "#### Twitter Scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c012265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
