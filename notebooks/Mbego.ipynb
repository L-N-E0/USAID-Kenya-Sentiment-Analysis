{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0bd178",
   "metadata": {},
   "source": [
    "#### â€“ Data Collection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸ“Œ Setup\n",
    "!pip install snscrape beautifulsoup4 requests --quiet\n",
    "\n",
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"data/mbego raw data\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77fec9",
   "metadata": {},
   "source": [
    "#### Twitter Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search query\n",
    "query = \"(USAID funding OR donor cuts OR USAID health) (Kenya) lang:en since:2023-01-01 until:2024-12-31\"\n",
    "\n",
    "tweets = []\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "    if i > 1000:  # limit to 1000 for testing\n",
    "        break\n",
    "    tweets.append({\n",
    "        \"date\": tweet.date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"username\": tweet.user.username,\n",
    "        \"text\": tweet.content\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_tweets = pd.DataFrame(tweets)\n",
    "\n",
    "# Save to file\n",
    "tweets_path = os.path.join(output_dir, \"tweets_usaid_kenya.json\")\n",
    "df_tweets.to_json(tweets_path, orient=\"records\", lines=True)\n",
    "print(f\"âœ… Saved {len(df_tweets)} tweets to: {tweets_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccaeeb",
   "metadata": {},
   "source": [
    "News Headline Scraping (Basic Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from The Standard (adjust for real pages)\n",
    "url = \"https://www.standardmedia.co.ke/topic/usaid\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Collect headlines\n",
    "articles = soup.find_all(\"h2\")  # You may need to update this tag\n",
    "news_data = [{\"headline\": a.text.strip()} for a in articles if a.text.strip()]\n",
    "\n",
    "# Save to JSON\n",
    "df_news = pd.DataFrame(news_data)\n",
    "news_path = os.path.join(output_dir, \"news_usaid_kenya.json\")\n",
    "df_news.to_json(news_path, orient=\"records\", lines=True)\n",
    "print(f\"âœ… Saved {len(df_news)} news headlines to: {news_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
