{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f35898",
   "metadata": {},
   "source": [
    "### USAUD FUNDING CUTS SENTIMENT ANALYSIS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b33964",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68823a",
   "metadata": {},
   "source": [
    ".....(light intro text)\n",
    ".....(TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15690d",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0c730",
   "metadata": {},
   "source": [
    "This is data preparation phase for the project. The dataset used here is compiled from two primary sources: Reddit (via web scraping) and NewsAPI (via API calls). Each contributor collected data independently from these platforms, targeting relevant topics for analysis. Below, we begin by importing the collected datasets, merging them, and performing initial cleaning steps to prepare the data for further exploration and modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c78cbe",
   "metadata": {},
   "source": [
    "#### Data Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92f453",
   "metadata": {},
   "source": [
    "##### news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5fbe550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Agatha_news.csv:\n",
      "['keyword', 'source', 'author', 'title', 'description', 'content', 'publishedAt', 'url']\n",
      "--------------------------------------------------\n",
      "Columns in cecilia.newsapi.csv:\n",
      "['keyword', 'source', 'title', 'description', 'url', 'publishedAt']\n",
      "--------------------------------------------------\n",
      "Columns in gnews_usaid_kenya_full.csv:\n",
      "['title', 'url', 'published_date', 'source', 'text']\n",
      "--------------------------------------------------\n",
      "Columns in gnews_usaid_kenya_full_en_sw.csv:\n",
      "['title', 'url', 'published_date', 'source', 'language', 'text']\n",
      "--------------------------------------------------\n",
      "Columns in leo_newsapi_articles.csv:\n",
      "['source', 'author', 'title', 'description', 'content', 'url', 'published_at']\n",
      "--------------------------------------------------\n",
      "Columns in leo_newsapi_articles_enriched.csv:\n",
      "['source', 'author', 'title', 'description', 'content', 'url', 'published_at', 'full_text']\n",
      "--------------------------------------------------\n",
      "Columns in Mbego_news_usaid_kenya_fulltext.csv:\n",
      "['source', 'author', 'title', 'description', 'url', 'publishedAt', 'summary', 'full_text']\n",
      "--------------------------------------------------\n",
      "Columns in Mbego_news_usaid_kenya_recent.csv:\n",
      "['source', 'author', 'title', 'description', 'url', 'publishedAt', 'content']\n",
      "--------------------------------------------------\n",
      "Columns in ruth_news.csv:\n",
      "['Unnamed: 0', 'source', 'title', 'description', 'content', 'url', 'publishedAt']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to your news_data folder\n",
    "folder_path = r'N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\news_data'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Read and display columns for each CSV file\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=0)  # Read only headers\n",
    "        print(f\"Columns in {file}:\")\n",
    "        print(list(df.columns))\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cccdc",
   "metadata": {},
   "source": [
    "##### reddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6b954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Agatha_reddit.csv:\n",
      "['title', 'selftext', 'subreddit', 'author', 'created_utc', 'url', 'score', 'num_comments', 'keyword']\n",
      "--------------------------------------------------\n",
      "Columns in cecilia.redditsubs.csv:\n",
      "['subreddit', 'keyword', 'title', 'text', 'date_posted', 'upvotes', 'comments', 'url', 'permalink']\n",
      "--------------------------------------------------\n",
      "Columns in cecilia.reddit_nbo_ke_africa.csv:\n",
      "['subreddit', 'keyword', 'title', 'text', 'date_posted', 'upvotes', 'comments', 'url', 'permalink']\n",
      "--------------------------------------------------\n",
      "Columns in leo_reddit_posts.csv:\n",
      "['subreddit', 'search_term', 'title', 'text', 'created_utc', 'created_date', 'score', 'num_comments', 'permalink', 'url']\n",
      "--------------------------------------------------\n",
      "Columns in Mbego_reddit_usaid_kenya.csv:\n",
      "['title', 'score', 'url', 'created', 'subreddit', 'selftext']\n",
      "--------------------------------------------------\n",
      "Columns in Mbego_reddit_usaid_kenya2.csv:\n",
      "['title', 'score', 'url', 'created', 'subreddit', 'selftext']\n",
      "--------------------------------------------------\n",
      "Columns in reddit_usaid_sentiment.csv:\n",
      "['subreddit', 'title', 'score', 'url', 'created_utc', 'num_comments', 'selftext']\n",
      "--------------------------------------------------\n",
      "Columns in ruth_reddit.csv:\n",
      "['Unnamed: 0', 'subreddit', 'title', 'score', 'url', 'created_utc', 'num_comments', 'selftext']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to your news_data folder\n",
    "folder_path = r'N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\reddit_data'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Read and display columns for each CSV file\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=0)  # Read only headers\n",
    "        print(f\"Columns in {file}:\")\n",
    "        print(list(df.columns))\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22927d",
   "metadata": {},
   "source": [
    "#### Data Merging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da281b",
   "metadata": {},
   "source": [
    "##### news_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing all News CSVs\n",
    "folder_path = 'news_data'\n",
    "\n",
    "# Final save location\n",
    "save_path = r\"N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\processed\"\n",
    "\n",
    "# All .csv files in the news_data folder\n",
    "news_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Define the final standardized columns\n",
    "standard_news_cols = [\n",
    "    'keyword', 'source', 'author', 'title', 'description', 'content',\n",
    "    'summary', 'full_text', 'publishedAt', 'url', 'language'\n",
    "]\n",
    "\n",
    "# Create empty master DataFrame\n",
    "merged_news_df = pd.DataFrame(columns=standard_news_cols)\n",
    "\n",
    "# Loop through each file\n",
    "for file in news_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename columns to match standard\n",
    "        df.rename(columns={\n",
    "            'published_at': 'publishedAt',\n",
    "            'published_date': 'publishedAt',\n",
    "            'text': 'content',\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Add missing columns\n",
    "        for col in standard_news_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = pd.NA\n",
    "\n",
    "        # Align column order\n",
    "        df = df[standard_news_cols]\n",
    "\n",
    "        # Add to master DataFrame\n",
    "        merged_news_df = pd.concat([merged_news_df, df], ignore_index=True)\n",
    "\n",
    "        print(f\"✅ Merged: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file}: {e}\")\n",
    "\n",
    "# Save merged file\n",
    "output_path = os.path.join(save_path, 'all_news_merged.csv')\n",
    "merged_news_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ All News files merged and saved to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3bbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e93497b",
   "metadata": {},
   "source": [
    "##### reddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# all Reddit CSVs\n",
    "folder_path = 'N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\raw\\reddit_data' \n",
    "reddit_files = [\n",
    "    'Agatha_reddit.csv',\n",
    "    'cecilia.redditsubs.csv',\n",
    "    'cecilia.reddit_nbo_ke_africa.csv',\n",
    "    'leo_reddit_posts.csv',\n",
    "    'Mbego_reddit_usaid_kenya.csv',\n",
    "    'Mbego_reddit_usaid_kenya2.csv',\n",
    "    'reddit_usaid_sentiment.csv',\n",
    "    'ruth_reddit.csv'\n",
    "]\n",
    "\n",
    "# standard columns\n",
    "standard_cols = [\n",
    "    'title', 'selftext', 'subreddit', 'author', 'created_utc',\n",
    "    'created_date', 'score', 'num_comments', 'keyword', 'search_term',\n",
    "    'date_posted', 'upvotes', 'comments', 'url', 'permalink'\n",
    "]\n",
    "\n",
    "merged_df = pd.DataFrame(columns=standard_cols)\n",
    "\n",
    "# Load and align each file\n",
    "for file in reddit_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename common variations manually if needed\n",
    "        df.rename(columns={\n",
    "            'text': 'selftext',\n",
    "            'created': 'created_utc',\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Add missing columns with NaNs\n",
    "        for col in standard_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = pd.NA\n",
    "\n",
    "        # Keep only standard columns (in that order)\n",
    "        df = df[standard_cols]\n",
    "\n",
    "        # Append to master DataFrame\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "        print(f\"Merged: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {file}: {e}\")\n",
    "\n",
    "# Save final merged file\n",
    "merged_df.to_csv(os.path.join(save_path, 'mbego_all_reddit_merged.csv'), index=False)\n",
    "print(\"\\n✅ All Reddit files merged and saved to 'all_reddit_merged.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b314281",
   "metadata": {},
   "source": [
    "#### Data Understanding "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtascnce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
