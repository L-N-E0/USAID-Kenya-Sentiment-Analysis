{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0bd178",
   "metadata": {},
   "source": [
    "### Data Collection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e9414",
   "metadata": {},
   "source": [
    "#### 1. Reddit Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d192eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 17 Reddit posts to:\n",
      "   üìÑ CSV:  N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\\reddit_usaid_kenya.csv\n",
      "   üìÑ JSON: N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\\reddit_usaid_kenya.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Custom output directory\n",
    "output_dir = r\"N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Replace these with your credentials\n",
    "client_id = 'uRoMjHrYqBAUjIFZgJilRg'\n",
    "client_secret = 'dQkm_47qP1qUUzOdvO8piv5AP--L0A'\n",
    "user_agent = 'usaidKenyaScraper by u/IngenuityStunning997'\n",
    "\n",
    "# ‚úÖ Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# ‚úÖ Define search parameters\n",
    "subreddits = ['Kenya', 'development', 'worldnews']\n",
    "query = \"USAID funding OR donor cuts OR USAID health\"\n",
    "limit = 1000\n",
    "\n",
    "# ‚úÖ Collect posts\n",
    "posts = []\n",
    "for subreddit in subreddits:\n",
    "    for submission in reddit.subreddit(subreddit).search(query, limit=limit):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'score': submission.score,\n",
    "            'url': submission.url,\n",
    "            'created': pd.to_datetime(submission.created_utc, unit='s'),\n",
    "            'subreddit': submission.subreddit.display_name,\n",
    "            'selftext': submission.selftext\n",
    "        })\n",
    "\n",
    "# ‚úÖ Convert to DataFrame\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# ‚úÖ Save as CSV and JSON\n",
    "filename = \"reddit_usaid_kenya\"\n",
    "csv_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "json_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "df.to_json(json_path, orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(df)} Reddit posts to:\")\n",
    "print(f\"   üìÑ CSV:  {csv_path}\")\n",
    "print(f\"   üìÑ JSON: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66baf293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 163 Reddit posts to: output\\reddit_usaid_kenya.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Replace these with your credentials\n",
    "client_id = 'uRoMjHrYqBAUjIFZgJilRg'\n",
    "client_secret = 'dQkm_47qP1qUUzOdvO8piv5AP--L0A'\n",
    "user_agent = 'usaidKenyaScraper by u/IngenuityStunning997'\n",
    "\n",
    "# Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Define your search parameters\n",
    "subreddits = ['Kenya', 'development', 'worldnews']  # Can add more\n",
    "query = \"USAID funding OR donor cuts OR health\"\n",
    "limit = 1000\n",
    "\n",
    "# Fetch posts\n",
    "posts = []\n",
    "for subreddit in subreddits:\n",
    "    for submission in reddit.subreddit(subreddit).search(query, limit=limit):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'score': submission.score,\n",
    "            'url': submission.url,\n",
    "            'created': pd.to_datetime(submission.created_utc, unit='s'),\n",
    "            'subreddit': submission.subreddit.display_name,\n",
    "            'selftext': submission.selftext\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Save to CSV or JSON\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "filename = \"reddit_usaid_kenya\"\n",
    "csv_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Saved {len(df)} Reddit posts to: {csv_path}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86dd0d",
   "metadata": {},
   "source": [
    "#### 2. NewsAPI.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f26852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Saved 27 news articles to: N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\\news_usaid_kenya_recent.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# üìå Your NewsAPI key\n",
    "api_key = \"92ec9d796b7d493295eb56be553d8208\"  # Replace with your actual key\n",
    "\n",
    "# ‚úÖ Limit to the last 30 days\n",
    "today = datetime.today()\n",
    "from_date = (today - timedelta(days=29)).strftime('%Y-%m-%d')\n",
    "to_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# üîç Search parameters\n",
    "params = {\n",
    "    'q': 'USAID Kenya',\n",
    "    'from': from_date,\n",
    "    'to': to_date,\n",
    "    'language': 'en',\n",
    "    'sortBy': 'relevancy',\n",
    "    'pageSize': 100\n",
    "}\n",
    "\n",
    "# ‚úÖ Headers to avoid 426 errors\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0',\n",
    "    'X-Api-Key': api_key\n",
    "}\n",
    "\n",
    "# üåê Endpoint\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# üìÅ Output directory\n",
    "output_dir = r\"N:\\Moringa\\afterM\\Leo NLP 004 USAID 01.06.2025\\USAID-Kenya-Sentiment-Analysis\\data\\mj 001 raw data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# üì• Fetch and process articles\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    articles = response.json().get(\"articles\", [])\n",
    "    news_data = []\n",
    "\n",
    "    for article in articles:\n",
    "        news_data.append({\n",
    "            \"source\": article[\"source\"][\"name\"],\n",
    "            \"author\": article[\"author\"],\n",
    "            \"title\": article[\"title\"],\n",
    "            \"description\": article[\"description\"],\n",
    "            \"url\": article[\"url\"],\n",
    "            \"publishedAt\": article[\"publishedAt\"],\n",
    "            \"content\": article[\"content\"]\n",
    "        })\n",
    "\n",
    "    # Save to DataFrame and CSV\n",
    "    df_news = pd.DataFrame(news_data)\n",
    "    filename = f\"news_usaid_kenya_recent\"\n",
    "    csv_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "    df_news.to_csv(csv_path, index=False)\n",
    "    print(f\"üì∞ Saved {len(df_news)} news articles to: {csv_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Failed to fetch articles. Status code: {response.status_code}\")\n",
    "    print(f\"üí° Response content: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d63da9",
   "metadata": {},
   "source": [
    "Combine NewsAPI (to get article URLs) with newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccaeeb",
   "metadata": {},
   "source": [
    "News Headline Scraping (Basic Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from The Standard (adjust for real pages)\n",
    "url = \"https://www.standardmedia.co.ke/topic/usaid\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Collect headlines\n",
    "articles = soup.find_all(\"h2\")  # You may need to update this tag\n",
    "news_data = [{\"headline\": a.text.strip()} for a in articles if a.text.strip()]\n",
    "\n",
    "# Save to JSON\n",
    "df_news = pd.DataFrame(news_data)\n",
    "news_path = os.path.join(output_dir, \"news_usaid_kenya.json\")\n",
    "df_news.to_json(news_path, orient=\"records\", lines=True)\n",
    "print(f\"‚úÖ Saved {len(df_news)} news headlines to: {news_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490cca4",
   "metadata": {},
   "source": [
    "#### Twitter Scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# === Your Bearer Token ===\n",
    "BEARER_TOKEN = \"YOUR_BEARER_TOKEN_HERE\"\n",
    "\n",
    "# === Query and output settings ===\n",
    "query = \"(USAID funding OR donor cuts OR USAID health) Kenya lang:en\"\n",
    "output_dir = \"./twitter_data\"  # Customize your storage directory\n",
    "filename = \"tweets_usaid_kenya\"  # Filename without extension\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure dir exists\n",
    "\n",
    "# === Twitter API v2 endpoint and headers ===\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "headers = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\n",
    "\n",
    "# === Search parameters ===\n",
    "params = {\n",
    "    \"query\": query,\n",
    "    \"max_results\": 100,\n",
    "    \"tweet.fields\": \"created_at,author_id,text\",\n",
    "}\n",
    "\n",
    "# === Collect tweets ===\n",
    "all_tweets = []\n",
    "\n",
    "for _ in range(10):  # Up to 1000 tweets\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(\"‚ùå Error:\", response.status_code, response.text)\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    for tweet in data.get(\"data\", []):\n",
    "        all_tweets.append({\n",
    "            \"date\": tweet[\"created_at\"],\n",
    "            \"author_id\": tweet[\"author_id\"],\n",
    "            \"text\": tweet[\"text\"]\n",
    "        })\n",
    "\n",
    "    next_token = data.get(\"meta\", {}).get(\"next_token\")\n",
    "    if not next_token:\n",
    "        break\n",
    "    params[\"next_token\"] = next_token\n",
    "    time.sleep(1)  # Prevent rate limit\n",
    "\n",
    "# === Save collected tweets ===\n",
    "if all_tweets:\n",
    "    df_tweets = pd.DataFrame(all_tweets)\n",
    "\n",
    "    # Save as JSON\n",
    "    json_path = os.path.join(output_dir, f\"{filename}.json\")\n",
    "    df_tweets.to_json(json_path, orient=\"records\", lines=True)\n",
    "    print(f\"‚úÖ Saved JSON: {json_path}\")\n",
    "\n",
    "    # Optional: Save as CSV\n",
    "    csv_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "    df_tweets.to_csv(csv_path, index=False)\n",
    "    print(f\"üìÑ Also saved as CSV: {csv_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No tweets collected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
